{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crear copia del programa del taller anterior. Realizar cambios para construir un clasificador que pueda clasificar un nuevo documento a una clase particular usando Naive Bayes. Probar con los ejercicios realizados en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import functools\n",
    "import math\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(training, test):\n",
    "    categories = {}\n",
    "    for document in training:\n",
    "        if document['cat'] in categories:\n",
    "            categories[document['cat']]['count'] += 1\n",
    "        else:\n",
    "            categories[document['cat']] = {}\n",
    "            categories[document['cat']]['count'] = 1\n",
    "            categories[document['cat']]['tokens'] = {}\n",
    "            \n",
    "    for category in categories:\n",
    "        categories[category]['probability'] = categories[category]['count'] / len(training)\n",
    "        \n",
    "    for document in training:\n",
    "        document['tokens'] = WordPunctTokenizer().tokenize(document['doc'])\n",
    "    \n",
    "    for document in training:\n",
    "        for token in document['tokens']:\n",
    "            if token in categories[document['cat']]['tokens']:\n",
    "                categories[document['cat']]['tokens'][token] += 1\n",
    "            else:\n",
    "                categories[document['cat']]['tokens'][token] = 1\n",
    "\n",
    "    vocabulary = [y for y in {token: 1 for x in categories for token in categories[x]['tokens']}]\n",
    "    \n",
    "    test_tokens = {token: 1 for token in WordPunctTokenizer().tokenize(test)}\n",
    "    \n",
    "    test = {\n",
    "        'categories': {\n",
    "            category: {\n",
    "                'probability': 0,\n",
    "                'tokens': dict(test_tokens)\n",
    "            } for category in categories\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    for category in test['categories']:\n",
    "        for test_token in test_tokens:\n",
    "            try:\n",
    "                test['categories'][category]['tokens'][test_token] += categories[category]['tokens'][test_token]\n",
    "            except KeyError:\n",
    "                #print(f\"Skipping token '{test_token}' on category '{category}'\")\n",
    "                category\n",
    "    \n",
    "    for category in test['categories']:\n",
    "        test['categories'][category]['probability'] = categories[category]['probability']\n",
    "        test['categories'][category]['probability'] *= functools.reduce(lambda x, y: x * y,\n",
    "            [(test['categories'][category]['tokens'][token] / (len(categories[category]['tokens']) + len(vocabulary))) for token in test['categories'][category]['tokens']])\n",
    "    \n",
    "    maximum = 0\n",
    "    classified_category = ''\n",
    "    \n",
    "    for category in test['categories']:\n",
    "        if test['categories'][category]['probability'] > maximum:\n",
    "            maximum = test['categories'][category]['probability']\n",
    "            classified_category = category\n",
    "    \n",
    "    maximum *= 100\n",
    "    maximum = f\"{round(maximum, 4)}%\"\n",
    "\n",
    "    print(f\"The test document was classified on the '{classified_category}' category with a probability of {maximum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test document was classified on the '-' category with a probability of 0.0002%\n"
     ]
    }
   ],
   "source": [
    "training_documents = [\n",
    "    {'cat': '-', 'doc': 'just plain boring'},\n",
    "    {'cat': '-', 'doc': 'entirely predictable and lacks energy'},\n",
    "    {'cat': '-', 'doc': 'no surprises and very few laughs'},\n",
    "    {'cat': '+', 'doc': 'very powerful'},\n",
    "    {'cat': '+', 'doc': 'the most fun film of the summer'}\n",
    "]\n",
    "\n",
    "test = 'predictable with no fun'\n",
    "\n",
    "naive_bayes(training_documents, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test document was classified on the 'China' category with a probability of 0.45%\n"
     ]
    }
   ],
   "source": [
    "training_documents = [\n",
    "    {'cat': 'China', 'doc': 'Chinese Beijing Chinese'},\n",
    "    {'cat': 'China', 'doc': 'Chinese Chinese Shanghai'},\n",
    "    {'cat': 'China', 'doc': 'Chinese Macao'},\n",
    "    {'cat': 'Japan', 'doc': 'Tokyo Japan Chinese'}\n",
    "]\n",
    "\n",
    "test = 'Chinese Chinese Chinese Tokyo Japan'\n",
    "\n",
    "naive_bayes(training_documents, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Basado en la función que implementaron para calcular el tf (conteo de términos para el modelo de lenguaje), calcular el tf-idf y armar los vectores de documentos basados en tfidf y luego normalizar para que un vector de documentos tenga una magnitud de 1. Implementar la similitud coseno entre los dos vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frecuencia de terminos en el documento dado\n",
    "def tf_d(document):\n",
    "    tokens = {}\n",
    "    for token in WordPunctTokenizer().tokenize(document):\n",
    "        tokens[token] = 1 if token not in tokens else tokens[token] + 1\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_t(documents, term):\n",
    "    return len(\n",
    "        [\n",
    "            lst for lst in [\n",
    "            [\n",
    "                token for token in WordPunctTokenizer().tokenize(document)] for document in documents\n",
    "            ] if term in lst\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(documents):\n",
    "    vocabulary = {token: 0 for token in[token for document in documents for token in WordPunctTokenizer().tokenize(document)]}\n",
    "    df_t_vocabulary = dict(vocabulary)\n",
    "    \n",
    "    for token in df_t_vocabulary:\n",
    "        df_t_vocabulary[token] = df_t(documents, token)\n",
    "    \n",
    "    idf_t = dict(df_t_vocabulary)\n",
    "    \n",
    "    for token in idf_t:\n",
    "        idf_t[token] = math.log10(len(vocabulary) / idf_t[token])\n",
    "    \n",
    "    w = {}\n",
    "    \n",
    "    for item in range(len(documents)):\n",
    "        document_tf_d = tf_d(documents[item])\n",
    "        w[f\"Document {item}\"] = dict(vocabulary)\n",
    "        for token in document_tf_d:\n",
    "            w[f\"Document {item}\"][token] = math.log10(1 + document_tf_d[token]) * idf_t[token]\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents\n",
      "---------\n",
      "Chinese Beijing Chinese\n",
      "Chinese Chinese Shanghai\n",
      "Chinese Macao\n",
      "Tokyo Japan Chinese\n",
      "\n",
      "Documents tf_idf (w)\n",
      "--------------------\n",
      "          Document 0  Document 1  Document 2  Document 3\n",
      "Beijing     0.234247    0.000000    0.000000    0.000000\n",
      "Chinese     0.084017    0.084017    0.053009    0.053009\n",
      "Japan       0.000000    0.000000    0.000000    0.234247\n",
      "Macao       0.000000    0.000000    0.234247    0.000000\n",
      "Shanghai    0.000000    0.234247    0.000000    0.000000\n",
      "Tokyo       0.000000    0.000000    0.000000    0.234247\n"
     ]
    }
   ],
   "source": [
    "print('Documents')\n",
    "print('---------')\n",
    "documents = [document['doc'] for document in training_documents]\n",
    "[print(doc) for doc in documents]\n",
    "\n",
    "tf_idf_docs = tf_idf(documents)\n",
    "\n",
    "print('\\nDocuments tf_idf (w)')\n",
    "print('--------------------')\n",
    "print(pandas.DataFrame(tf_idf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors normalized\n",
      "------------------\n",
      "          Document 0  Document 1  Document 2  Document 3\n",
      "Beijing     0.941286    0.000000    0.000000    0.000000\n",
      "Chinese     0.337609    0.337609    0.220714    0.158004\n",
      "Japan       0.000000    0.000000    0.000000    0.698224\n",
      "Macao       0.000000    0.000000    0.975339    0.000000\n",
      "Shanghai    0.000000    0.941286    0.000000    0.000000\n",
      "Tokyo       0.000000    0.000000    0.000000    0.698224\n"
     ]
    }
   ],
   "source": [
    "for document in tf_idf_docs:\n",
    "    norm = math.sqrt(sum([x * x for x in [tf_idf_docs[document][x] for x in tf_idf_docs[document]]]))\n",
    "    for token in tf_idf_docs[document]:\n",
    "        tf_idf_docs[document][token] /= norm\n",
    "        \n",
    "print('Vectors normalized')\n",
    "print('------------------')\n",
    "print(pandas.DataFrame(tf_idf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector_A, vector_B):\n",
    "    \n",
    "    return sum([\n",
    "        element[0] * element[1]\n",
    "        for element in list(zip([vector_A[x] for x in vector_A], [vector_B[x] for x in vector_B]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine between 0 and 1:\n",
      "0.11398009621209179\n",
      "Cosine between 0 and 2:\n",
      "0.07451500289402613\n",
      "Cosine between 0 and 3:\n",
      "0.05334372216788929\n",
      "\n",
      "\n",
      "Cosine between 1 and 2:\n",
      "0.07451500289402613\n",
      "Cosine between 1 and 3:\n",
      "0.05334372216788929\n",
      "\n",
      "\n",
      "Cosine between 2 and 3:\n",
      "0.034873699389777406\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements = [tf_idf_docs[item] for item in tf_idf_docs]\n",
    "\n",
    "for idx, item in enumerate(elements):\n",
    "    for idy, element in enumerate(elements[idx + 1:]):\n",
    "        print(f\"Cosine between {idx} and {idy + idx + 1}:\")\n",
    "        print(cosine_similarity(elements[idx], elements[idy + idx + 1]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Con el programa del punto anterior, usando los mismos ejercicios usados en el punto 1 clasificar usando K vecinos más cercanos.\n",
    "\n",
    "En este punto se tomará el último documento de la lista y se comparará con los tres primeros, con `k = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test document is more likely to be in the same category as the 3 document, having a score of 0.0349:\n",
      "Chinese: 0.2207136602535519\n",
      "Beijing: 0.0\n",
      "Shanghai: 0.0\n",
      "Macao: 0.9753386489714635\n",
      "Tokyo: 0.0\n",
      "Japan: 0.0\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "\n",
    "docs = [tf_idf_docs[doc] for doc in tf_idf_docs]\n",
    "\n",
    "test = docs.pop(len(docs) - 1)\n",
    "\n",
    "maximum = (0, 0)\n",
    "\n",
    "for idx, item in enumerate(docs):\n",
    "    maximum = (idx, cosine_similarity(item, test))\n",
    "\n",
    "print(f\"Test document is more likely to be in the same category as the {maximum[0] + 1} document, having a score of {round(maximum[1], 4)}:\")\n",
    "\n",
    "for x in docs[maximum[0]]:\n",
    "    print(f\"{x}: {docs[maximum[0]][x]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
