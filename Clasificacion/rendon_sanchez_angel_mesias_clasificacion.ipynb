{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crear copia del programa del taller anterior. Realizar cambios para construir un clasificador que pueda clasificar un nuevo documento a una clase particular usando Naive Bayes. Probar con los ejercicios realizados en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import functools\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(training, test):\n",
    "    categories = {}\n",
    "    for document in training:\n",
    "        if document['cat'] in categories:\n",
    "            categories[document['cat']]['count'] += 1\n",
    "        else:\n",
    "            categories[document['cat']] = {}\n",
    "            categories[document['cat']]['count'] = 1\n",
    "            categories[document['cat']]['tokens'] = {}\n",
    "            \n",
    "    for category in categories:\n",
    "        categories[category]['probability'] = categories[category]['count'] / len(training)\n",
    "        \n",
    "    for document in training:\n",
    "        document['tokens'] = WordPunctTokenizer().tokenize(document['doc'])\n",
    "    \n",
    "    for document in training:\n",
    "        for token in document['tokens']:\n",
    "            if token in categories[document['cat']]['tokens']:\n",
    "                categories[document['cat']]['tokens'][token] += 1\n",
    "            else:\n",
    "                categories[document['cat']]['tokens'][token] = 1\n",
    "\n",
    "    vocabulary = [y for y in {token: 1 for x in categories for token in categories[x]['tokens']}]\n",
    "    \n",
    "    test_tokens = {token: 1 for token in WordPunctTokenizer().tokenize(test)}\n",
    "    \n",
    "    test = {\n",
    "        'categories': {\n",
    "            category: {\n",
    "                'probability': 0,\n",
    "                'tokens': dict(test_tokens)\n",
    "            } for category in categories\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    for category in test['categories']:\n",
    "        for test_token in test_tokens:\n",
    "            try:\n",
    "                test['categories'][category]['tokens'][test_token] += categories[category]['tokens'][test_token]\n",
    "            except KeyError:\n",
    "                #print(f\"Skipping token '{test_token}' on category '{category}'\")\n",
    "                category\n",
    "    \n",
    "    for category in test['categories']:\n",
    "        test['categories'][category]['probability'] = categories[category]['probability']\n",
    "        test['categories'][category]['probability'] *= functools.reduce(lambda x, y: x * y,\n",
    "            [(test['categories'][category]['tokens'][token] / (len(categories[category]['tokens']) + len(vocabulary))) for token in test['categories'][category]['tokens']])\n",
    "    \n",
    "    maximum = 0\n",
    "    classified_category = ''\n",
    "    \n",
    "    for category in test['categories']:\n",
    "        if test['categories'][category]['probability'] > maximum:\n",
    "            maximum = test['categories'][category]['probability']\n",
    "            classified_category = category\n",
    "    \n",
    "    maximum *= 100\n",
    "    maximum = f\"{round(maximum, 4)}%\"\n",
    "\n",
    "    print(f\"The test document was classified on the '{classified_category}' category with a probability of {maximum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test document was classified on the '-' category with a probability of 0.0002%\n"
     ]
    }
   ],
   "source": [
    "training_documents = [\n",
    "    {'cat': '-', 'doc': 'just plain boring'},\n",
    "    {'cat': '-', 'doc': 'entirely predictable and lacks energy'},\n",
    "    {'cat': '-', 'doc': 'no surprises and very few laughs'},\n",
    "    {'cat': '+', 'doc': 'very powerful'},\n",
    "    {'cat': '+', 'doc': 'the most fun film of the summer'}\n",
    "]\n",
    "\n",
    "test = 'predictable with no fun'\n",
    "\n",
    "naive_bayes(training_documents, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test document was classified on the 'China' category with a probability of 0.45%\n"
     ]
    }
   ],
   "source": [
    "training_documents = [\n",
    "    {'cat': 'China', 'doc': 'Chinese Beijing Chinese'},\n",
    "    {'cat': 'China', 'doc': 'Chinese Chinese Shanghai'},\n",
    "    {'cat': 'China', 'doc': 'Chinese Macao'},\n",
    "    {'cat': 'Japan', 'doc': 'Tokyo Japan Chinese'}\n",
    "]\n",
    "\n",
    "test = 'Chinese Chinese Chinese Tokyo Japan'\n",
    "\n",
    "naive_bayes(training_documents, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Basado en la función que implementaron para calcular el tf (conteo de términos para el modelo de lenguaje), calcular el tf-idf y armar los vectores de documentos basados en tfidf y luego normalizar para que un vector de documentos tenga una magnitud de 1. Implementar la similitud coseno entre los dos vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chinese': 4, 'Beijing': 1, 'Shanghai': 1, 'Macao': 1, 'Tokyo': 1, 'Japan': 1}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Frecuencia de terminos en el documento dado\n",
    "def tf_d(document):\n",
    "    tokens = {}\n",
    "    for token in WordPunctTokenizer().tokenize(document):\n",
    "        tokens[token] = 1 if token not in tokens else tokens[token] + 1\n",
    "    return(tokens)\n",
    "    \n",
    "def df_t(documents, term):\n",
    "    return len(\n",
    "        [\n",
    "            lst for lst in [\n",
    "            [\n",
    "                token for token in WordPunctTokenizer().tokenize(document)] for document in documents\n",
    "            ] if term in lst\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def tf_idf(documents):\n",
    "    vocabulary = {token: 0 for token in[token for document in documents for token in WordPunctTokenizer().tokenize(document)]}\n",
    "    df_t_vocabulary = dict(vocabulary)\n",
    "    for token in df_t_vocabulary:\n",
    "        df_t_vocabulary[token] = df_t(documents, token)\n",
    "    print(df_t_vocabulary)\n",
    "\n",
    "print(tf_idf([document['doc'] for document in training_documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
