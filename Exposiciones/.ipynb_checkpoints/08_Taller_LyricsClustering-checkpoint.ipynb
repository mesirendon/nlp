{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrmSHF2ZjtSj"
   },
   "source": [
    "#Taller Clustering en letras de canciones \n",
    "###Sergio Lopez Martinez - PLN - 2019-l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSR8TYcVjuZU"
   },
   "source": [
    "##Importar los conjuntos de datos que utilizaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jse1NT33kAXl"
   },
   "source": [
    "Tenemos 2 conjuntos, el primero contiene las letras originales, el segundo  de letras luego de ser procesadas con PartOfSpeech utilizando Freeling: http://nlp.lsi.upc.edu/freeling/demo/demo.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3035,
     "status": "ok",
     "timestamp": 1562899430072,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "edGpdR0cLN-9",
    "outputId": "bc8a8173-4429-4ef4-aeb0-31c32028e173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-12 02:43:47--  https://drive.google.com/uc?export=download&id=1sB0VJnqBR5PbpNLfW73UeZFIbwlL3cv4\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.141.100, 74.125.141.101, 74.125.141.139, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.141.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-08-40-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nfogtup9pafgnc3506berhan8ie0u2m9/1562896800000/10519510251368043159/*/1sB0VJnqBR5PbpNLfW73UeZFIbwlL3cv4?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-07-12 02:43:48--  https://doc-08-40-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nfogtup9pafgnc3506berhan8ie0u2m9/1562896800000/10519510251368043159/*/1sB0VJnqBR5PbpNLfW73UeZFIbwlL3cv4?e=download\n",
      "Resolving doc-08-40-docs.googleusercontent.com (doc-08-40-docs.googleusercontent.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
      "Connecting to doc-08-40-docs.googleusercontent.com (doc-08-40-docs.googleusercontent.com)|74.125.141.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10191 (10.0K) [application/x-zip-compressed]\n",
      "Saving to: ‘Lyrics.zip’\n",
      "\n",
      "\r",
      "Lyrics.zip            0%[                    ]       0  --.-KB/s               \r",
      "Lyrics.zip          100%[===================>]   9.95K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-07-12 02:43:48 (65.7 MB/s) - ‘Lyrics.zip’ saved [10191/10191]\n",
      "\n",
      "Archive:  Lyrics.zip\n",
      "   creating: Lyrics/\n",
      "  inflating: Lyrics/Abandonado - Cafe Tacvba.txt  \n",
      "  inflating: Lyrics/Abandonado - Efren David.txt  \n",
      "  inflating: Lyrics/Abduccion - Voodoo Zombie.txt  \n",
      "  inflating: Lyrics/Ahora me llama - Bad Bunny.txt  \n",
      "  inflating: Lyrics/Amorfoda - Bad Bunny.txt  \n",
      "  inflating: Lyrics/Babel - Heroes del silencio.txt  \n",
      "  inflating: Lyrics/Babilonia - Tego Calderon.txt  \n",
      "  inflating: Lyrics/Cabalgata - Roberto Carlos.txt  \n",
      "  inflating: Lyrics/Caballero - Chino y Nacho.txt  \n",
      "  inflating: Lyrics/Como mirarte - Sebastian Yatra.txt  \n",
      "  inflating: Lyrics/Dados cargados - Conjunto rio grande.txt  \n",
      "  inflating: Lyrics/Dale caliente - Daddy Yankee.txt  \n",
      "  inflating: Lyrics/Dale castigo - Hector el bambino.txt  \n",
      "  inflating: Lyrics/El amor de Dios es maravilloso - Ninos.txt  \n",
      "  inflating: Lyrics/En los anos 1600 - Joe Arroyo.txt  \n",
      "  inflating: Lyrics/Pijamas - Babasonicos.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget -O Lyrics.zip \"https://drive.google.com/uc?export=download&id=1sB0VJnqBR5PbpNLfW73UeZFIbwlL3cv4\"\n",
    "!unzip Lyrics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2880,
     "status": "ok",
     "timestamp": 1562899435185,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "5V6AJnBPNez1",
    "outputId": "7dfe3c08-a770-4d18-dab3-0cbc7d4a084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-12 02:43:52--  https://drive.google.com/uc?export=download&id=1hv4kKcpgZRy79L9PKESrK6j6MClY-FIw\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.141.139, 74.125.141.138, 74.125.141.100, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.141.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-40-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0e0dcrdj25hfngeu4froqs4pfrubr87t/1562896800000/10519510251368043159/*/1hv4kKcpgZRy79L9PKESrK6j6MClY-FIw?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-07-12 02:43:53--  https://doc-0k-40-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0e0dcrdj25hfngeu4froqs4pfrubr87t/1562896800000/10519510251368043159/*/1hv4kKcpgZRy79L9PKESrK6j6MClY-FIw?e=download\n",
      "Resolving doc-0k-40-docs.googleusercontent.com (doc-0k-40-docs.googleusercontent.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
      "Connecting to doc-0k-40-docs.googleusercontent.com (doc-0k-40-docs.googleusercontent.com)|74.125.141.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15406 (15K) [application/x-zip-compressed]\n",
      "Saving to: ‘POS-Freeling.zip’\n",
      "\n",
      "\r",
      "POS-Freeling.zip      0%[                    ]       0  --.-KB/s               \r",
      "POS-Freeling.zip    100%[===================>]  15.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-07-12 02:43:53 (73.2 MB/s) - ‘POS-Freeling.zip’ saved [15406/15406]\n",
      "\n",
      "Archive:  POS-Freeling.zip\n",
      "   creating: POS-Freeling/\n",
      "  inflating: POS-Freeling/Abandonado - Cafe Tacvba.txt  \n",
      "  inflating: POS-Freeling/Abandonado - Efren David.txt  \n",
      "  inflating: POS-Freeling/Abduccion - Voodoo Zombie.txt  \n",
      "  inflating: POS-Freeling/Ahora me llama - Bad Bunny.txt  \n",
      "  inflating: POS-Freeling/Amorfoda - Bad Bunny.txt  \n",
      "  inflating: POS-Freeling/Babel - Heroes del silencio.txt  \n",
      "  inflating: POS-Freeling/Babilonia - Tego Calderon.txt  \n",
      "  inflating: POS-Freeling/Cabalgata - Roberto Carlos.txt  \n",
      "  inflating: POS-Freeling/Caballero - Chino y Nacho.txt  \n",
      "  inflating: POS-Freeling/Como mirarte - Sebastian Yatra.txt  \n",
      "  inflating: POS-Freeling/Dados cargados - Conjunto rio grande.txt  \n",
      "  inflating: POS-Freeling/Dale caliente - Daddy Yankee.txt  \n",
      "  inflating: POS-Freeling/Dale castigo - Hector el bambino.txt  \n",
      "  inflating: POS-Freeling/El amor de Dios es maravilloso - Ninos.txt  \n",
      "  inflating: POS-Freeling/En los anos 1600 - Joe Arroyo.txt  \n",
      "  inflating: POS-Freeling/Pijamas - Babasonicos.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget -O POS-Freeling.zip \"https://drive.google.com/uc?export=download&id=1hv4kKcpgZRy79L9PKESrK6j6MClY-FIw\"\n",
    "!unzip POS-Freeling.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNBlCZHLjuGD"
   },
   "source": [
    "##Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElUykBzOllam"
   },
   "source": [
    "Creamos un directorio para relizar la lematizacion de las letras originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oavbgv4nklHP"
   },
   "outputs": [],
   "source": [
    "!mkdir Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1562899440203,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "PcYII6pAIi1r",
    "outputId": "be0936cd-b6e3-48f3-e9a3-75b0f3bb150b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Lemma done succesfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "#Import files\n",
    "files = (glob.glob(\"Lyrics/*\"))\n",
    "\n",
    "#Initialize banned words.\n",
    "banned_words = {\n",
    "    \"yeah\": 1,\n",
    "    \"oh\": 1\n",
    "}\n",
    "\n",
    "#Clean all words from all files\n",
    "for file in files:\n",
    "    filename = file[file.rfind(\"/\") + 1:]\n",
    "    with open(file, 'r') as f:\n",
    "        write = \"\"\n",
    "        for line in f:\n",
    "            words = map(str, line.strip().split(' '))\n",
    "            for w in words:\n",
    "                if w not in banned_words.keys():\n",
    "                    if len(w) > 2:\n",
    "                        write += w.lower() + \" \"\n",
    "        write += '\\n'\n",
    "    with open(\"Lemma/\"+ filename, 'w') as l:\n",
    "        l.write(write)\n",
    "print (\"\\n\\nLemma done succesfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xS92eNramIjU"
   },
   "source": [
    "###Ahora con el siguiente script sobre el conjunto de datos preprocesado de PartOfSpeech, crea las palabras procesadas con las frecuencias que cada una tiene en su archivo de letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1_H0y1QOtCP"
   },
   "outputs": [],
   "source": [
    "!mkdir DataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1562899448992,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "XBKp-EENNGJJ",
    "outputId": "e9c6c30d-4824-41b7-f788-fe5ca898cdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DataJson done succesfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Import files\n",
    "files = (glob.glob(\"POS-Freeling/*\"))\n",
    "c = 0\n",
    "#Import Lemma files from Freeling Software\n",
    "for file in files:\n",
    "    d = {}\n",
    "    filename = file[file.rfind(\"/\") + 1:]\n",
    "    filename_small = filename.replace(\".txt\", \".json\")\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if(c % 3 == 1):\n",
    "                if line in d:\n",
    "                    d[line] += 1\n",
    "                else:\n",
    "                    d[line] = 1\n",
    "            c += 1\n",
    "    #Export data to json\n",
    "    with open(\"DataJson/\" + filename_small, 'w') as fp:\n",
    "        json.dump(d, fp, sort_keys=True, indent=4)\n",
    "\n",
    "print (\"\\n\\nDataJson done succesfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rRskFj7PGXc"
   },
   "outputs": [],
   "source": [
    "!mkdir Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoTucjcemwCb"
   },
   "source": [
    "## Removiendo stopwords\n",
    "Este archivo limpia las palabras que ya se han procesado en la Extracción de la parte de la letra de la parte de la letra (POS), eliminando todas las palabras clave encontradas en cada archivo de letras, utilizando una lista de palabras clave estándar que se encuentra en el paquete NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1369,
     "status": "ok",
     "timestamp": 1562899454804,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "bdeF67EaPTEs",
    "outputId": "01f3d156-bddc-4bd1-9502-6634f73a721f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1562899455741,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "j1W0DjBAPAD-",
    "outputId": "eadd29c2-7eca-4c9b-e394-40f41751793a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added all cleaned files to files/Cleaned\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Getting stopwords from nltk\n",
    "stop_words = set( stopwords.words('spanish') )\n",
    "\n",
    "# File path of processed words\n",
    "filepath = 'DataJson/'\n",
    "\n",
    "# Getting names of all the files in the lemma folder\n",
    "onlyfiles = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "\n",
    "# Looping all files in folder\n",
    "for filename in onlyfiles:\n",
    "\n",
    "    # Get the words of the file\n",
    "    data = json.load( open(filepath + filename) )\n",
    "    word_tokens = data.keys()\n",
    "\n",
    "    # Cutting the stopwords from the words array\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    # Creating a new file to save the new json with the original frequencies\n",
    "    new_dict = {}\n",
    "    for key in filtered_sentence:\n",
    "        new_dict[key] = data[key]\n",
    "\n",
    "    # Saving the file\n",
    "    with open('Cleaned/' + filename, 'w') as outfile:\n",
    "        json.dump(new_dict, outfile)\n",
    "\n",
    "print (\"Added all cleaned files to files/Cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1skNW56_nik1"
   },
   "source": [
    "##Calculo de tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTCW03TynoIy"
   },
   "source": [
    "Utilizando el conjunto anterior y el vectorizador tf-idf proporcionado por la libreria sklearn, podremos hacerlo de una manera sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1562899459344,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "Cgh3fh8eR9Wv",
    "outputId": "b54f517c-114f-4167-db48-fcedb86c67c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import files\n",
    "files = (glob.glob(\"Cleaned/*\"))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(files)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBF1YWMkoDv4"
   },
   "source": [
    "Nuestra matriz se encuentra en un formato de fila dispersa, debido a su composicion, recordemos que esta matiz contiene bastantes 0s por lo cual su espacio en memoria por sus dimensiones es elevado, por lo cual python utiliza este formato para disminuir dicho espacio de almacenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1562899460505,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "phbZCpbLZR_Q",
    "outputId": "f6ba32ef-095b-4b8d-882e-35d71b2e4c0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x60 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 95 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KO3Lt5RqoXsg"
   },
   "source": [
    "Sin embargo, con el atributo data nos permite ver los valores que no son 0 dentro de la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1562899462391,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "9xUBnpd7XEuK",
    "outputId": "9460279f-542d-4bb8-bd64-a618f376c74d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12070278, 0.3790147 , 0.3790147 , 0.3790147 , 0.3790147 ,\n",
       "       0.3790147 , 0.3790147 , 0.33007394, 0.12070278, 0.40086564,\n",
       "       0.40086564, 0.46030284, 0.46030284, 0.46030284, 0.14659017,\n",
       "       0.14659017, 0.40151773, 0.40151773, 0.40151773, 0.40151773,\n",
       "       0.40151773, 0.40151773, 0.1278692 , 0.1278692 , 0.55876914,\n",
       "       0.55876914, 0.55876914, 0.17794821, 0.17794821, 0.58111448,\n",
       "       0.58111448, 0.50607732, 0.18506441, 0.18506441, 0.60637457,\n",
       "       0.52807567, 0.52807567, 0.19310885, 0.19310885, 0.46030284,\n",
       "       0.46030284, 0.46030284, 0.40086564, 0.14659017, 0.40086564,\n",
       "       0.14659017, 0.55876914, 0.55876914, 0.55876914, 0.17794821,\n",
       "       0.17794821, 0.6737652 , 0.6737652 , 0.21457038, 0.21457038,\n",
       "       0.48778518, 0.48778518, 0.48778518, 0.48778518, 0.15534233,\n",
       "       0.15534233, 0.50243895, 0.50243895, 0.50243895, 0.43756087,\n",
       "       0.16000903, 0.16000903, 0.55876914, 0.55876914, 0.55876914,\n",
       "       0.17794821, 0.17794821, 0.55876914, 0.55876914, 0.55876914,\n",
       "       0.17794821, 0.17794821, 0.48778518, 0.48778518, 0.48778518,\n",
       "       0.48778518, 0.15534233, 0.15534233, 0.43840933, 0.43840933,\n",
       "       0.43840933, 0.43840933, 0.43840933, 0.13961786, 0.13961786,\n",
       "       0.58111448, 0.58111448, 0.50607732, 0.18506441, 0.18506441])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOaKBHatnRlR"
   },
   "source": [
    "##Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DXE_8rcopu_"
   },
   "source": [
    "Para facilidad en el clustering se utilizo CLUTO por su facilidad para evaluar el modelo en bisecciones repetidas con refinaciones(RBR), utilizando los datos del vector anteriormente obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf7e_P4OrWPJ"
   },
   "source": [
    "Como lo explican en el paper se utiliza RBR con funcion de optimizacion I2, obteniendo como resultado el vector de clustering que se utiliza en el siguiente codig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1562899464366,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "YHlQfmT-Xf_w",
    "outputId": "8f8f1139-e519-43b2-9741-c6b8dc5569fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster:  4\n",
      "funcionar\thacer\thuérfano\tllorar\tllover\tmaldecir\tmi_o\tnada\tpalabra\tplantar\t\n",
      "responder\tseco\tsin\tvisible\talguno\tdecir\tdesilusionar\tdios\tel\tir\t\n",
      "pertenecer\tsuerte\tvez\testar\tmal\tabandonar\tromper\ttrigo\tcalentura\tcama\t\n",
      "castigar\tcuerpo\tfigura_o\thasta\tmiedo\tpara\tsaciar\tse\tsentar\tsolo\t\n",
      "ver\ttener\tbellaco\tmeter\tte\tél\ttirar\tseguir\tque\tperro\t\n",
      "me\tle\tcastigo\tquerer\tser\t\n",
      "\n",
      "\n",
      "Cluster:  0\n",
      "cariño\tintención\tjugar\tme\tnadie\tsiempre\tsil_vado\tsilvando\tsin\tamigo\t\n",
      "caminar\tculpa\tfin\tfracaso\tmi\tperder\trecordar\ttriste\tabandonar\tsólo\t\n",
      "amor\testar\tpara\tusar\tvanesa\taquí\tconseguir\tdecir\tmujeriego\tpaciencia\t\n",
      "pensar\tperfecto\tpero\tperro\tsentar\tsentir\tese\tir\tnunca\tsolo\t\n",
      "tú\tcompañero\tquerer\ttener\tmami\tcontigo\tcaballero\tcon\tsaber\tser\t\n",
      "haber\tpor\t\n",
      "\n",
      "\n",
      "Cluster:  3\n",
      "ver\tvida\tbaba\tcrear\tcreer\tcual\tdeber\teste\tidea\tle\t\n",
      "mirar\tnariz\tparado\tpensar\tquerer\tquien\tuno\tvecino\tverde\talrededor\t\n",
      "ellos\tpor\tsalir\ttodo\tser\tlo\tmientras\tmorir\tde\tel\t\n",
      "señor\tabajo\tafuera\talto\tancho\tarriba\tperdón\tprofundo\tgrande\tmaravilloso\t\n",
      "él\tamor\testar\tque\ttan\tpoder\t\n",
      "\n",
      "\n",
      "Cluster:  5\n",
      "llevar\tlo\tmalo\tmejor\tmás\tsalir\tsin\tsoltero\tsquad\tte\t\n",
      "ye_h\tmío\ttener\tmanera\tresbalar\tvivo\tahora\tcomo\tdecir\tvida\t\n",
      "sólo\tquerer\tplaneta\tplantar\tprohibición\tpuerta\tquitar\tregresar\tsabiduría\tsaltar\t\n",
      "sed\tseparar\tsur\ttabaco\ttomar\tvaya\tvino\talguien\taquel\tde\t\n",
      "hombre\tmano\totro\tquién\tsu\tbabel\tel\tdormir\tempezar\tjuntar\t\n",
      "neutro\tnoche\tpijama\tproponer\trío\ttraer\tcosa\tcuento\teste\tinventar\t\n",
      "ocurrir\ttanto\ttu\tver\tnada\tpara\tentender\tpedir\tvez\túnico\t\n",
      "por\tme\tcon\thacer\tllamar\ttú\tnos\ttodo\t\n",
      "\n",
      "\n",
      "Cluster:  1\n",
      "trago\tverdad\tvez\tdía\thaber\thacer\thoy\tpor\tcomo\tsin\t\n",
      "cansar\thablar\tpasar\tsaber\ttruco\ttú\tmás\tnadie\tamor\tdolor\t\n",
      "ese\tlo\ttodo\tprecio\tpreparar\tpuro\trancho\tsaco\tseñor\ttanto\t\n",
      "tejano\ttener\ttenia\ttraición\tvoz\tcon\tcontrario\tjugada\trevisar\tterminar\t\n",
      "uno\tver\tdecir\tque\tsu\tsuerte\tdar\tle\tperder\tser\t\n",
      "el\tquerer\t\n",
      "\n",
      "\n",
      "Cluster:  2\n",
      "man\tme\tmir\tmás\tninguno\tpasar\tpero\tpesado\tpoder\trebulear\t\n",
      "sopa\ttarar\ttú\tvender\tcha_n\tdejar\tllegar\tmejor\tqué\ttiriri\t\n",
      "meter\tpaso\tser\ttener\tamor\taun\tbelleza\tbrillar\tconservar\tde\t\n",
      "dominar\tesperar\testrella\tgrandeza\thora\tlugar\tmi\tnacer\tsaber\tsol\t\n",
      "ver\tcabalgar\tdespués\tinstante\tnos\tnuestro\tque\tsin\tpor\tuno\t\n",
      "como\tel\tpara\t\n",
      "\n",
      "\n",
      "Cluster:  6\n",
      "día\tesperar\testar\thacer\thacia\timportar\tmorir\tmío\tojo\tpasajero\t\n",
      "pasar\tpoder\tpronto\tqué\tsonrisa\ttener\tvida\tdestino\tnos\tquerer\t\n",
      "como\tfundir\thasta\tlo\tmorena\toler\tpapi\tpiel\tquemar\trumba\t\n",
      "seguir\tsuela\ttela\tmas\tmover\tcalentar\tcandela\tcliente\tpara\tme\t\n",
      "caliente\tsalome\ttambién\ttirano\ttrato\ttú\tusted\tvenganza\tvivir\tafricano\t\n",
      "alma\taun\tchapalacate\tcuando\tescuchar\toír\tpor\ttierra\tverja\tdecir\t\n",
      "esclavitud\thistoria\tperpetuo\tnegro\tver\teste\tmirar\tsaber\tte\tel\t\n",
      "que\tle\tese\tser\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "h = {}\n",
    "files = (glob.glob(\"DataJson/*\"))\n",
    "\n",
    "files = sorted(files)\n",
    "\n",
    "# Cluster array\n",
    "cluster = [4,0,3,5,1,5,2,2,0,6,1,6,4,3,6,5]\n",
    "#cluster = vectors.data\n",
    "\n",
    "final = {}\n",
    "\n",
    "def print_cluster( id, cluster ):\n",
    "    print (\"Cluster: \", id)\n",
    "\n",
    "    row = \"\"\n",
    "    counter = 0\n",
    "    for item in cluster:\n",
    "        row += item[0] + \"\\t\"\n",
    "        counter += 1\n",
    "        if counter == 10:\n",
    "            print (row)\n",
    "            row = \"\"\n",
    "            counter = 0\n",
    "    print (row)\n",
    "    print (\"\\n\")\n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    h = json.load(open(file))\n",
    "    sorted_h = sorted(h.items(), key=operator.itemgetter(1))\n",
    "    x = sorted_h[-30:-1]\n",
    "    for w,v in x:\n",
    "        if cluster[counter] not in final:\n",
    "            final[cluster[counter]] = {}\n",
    "        else:\n",
    "            if w not in final[cluster[counter]]:\n",
    "                final[cluster[counter]][w] = 1\n",
    "            else:\n",
    "                final[cluster[counter]][w] += 1\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "\n",
    "for h in final:\n",
    "  print_cluster( h, sorted(final[h].items(), key=operator.itemgetter(1)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plHgHV30sFzI"
   },
   "source": [
    "Aqui se ve la separacion de los clusters obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGWwUEWyr8XB"
   },
   "source": [
    "Si lo desean pueden descargar el software y probar ustedes mismos con distintas letras de canciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fP1f8Qi5p7Jb"
   },
   "source": [
    "link software CLUTO: http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iw-5presjVnJ"
   },
   "source": [
    "Ejercicio sacado de: https://github.com/datovard/LyricsClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ci0J23SyEWdy"
   },
   "source": [
    "**Comentarios:** En este taller se utilizan las letras de 16 canciones de direntes géneros musicales, que varian desde reggaeton a balada pop, se utiliza clustering para agrupar las palabras en 6 clusters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "08_Taller_LyricsClustering.ipynb",
   "provenance": [
    {
     "file_id": "1bZmrSYuO7TRfeRaQdrcAl_vcs0WvDWpp",
     "timestamp": 1562889112346
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
