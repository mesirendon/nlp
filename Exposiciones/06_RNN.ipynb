{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6Fjk_4XeDwo"
   },
   "source": [
    "https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4\n",
    "\n",
    "https://www.youtube.com/watch?v=iMIWee_PXl8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX63Sx4UbTMx"
   },
   "source": [
    "# RNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3287,
     "status": "ok",
     "timestamp": 1562898065390,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "RzhlBUrEbNKi",
    "outputId": "99c9428b-0ad9-405f-f360-b77282225549"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1V5cM_0Qccex"
   },
   "source": [
    "### Preparaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3263,
     "status": "ok",
     "timestamp": 1562898065393,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "4v3atKeRbsKX",
    "outputId": "a0daa8bd-b797-49ea-f915-bd933460f9ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  ],\n",
       "        [0.01],\n",
       "        [0.02],\n",
       "        [0.03],\n",
       "        [0.04]],\n",
       "\n",
       "       [[0.01],\n",
       "        [0.02],\n",
       "        [0.03],\n",
       "        [0.04],\n",
       "        [0.05]],\n",
       "\n",
       "       [[0.02],\n",
       "        [0.03],\n",
       "        [0.04],\n",
       "        [0.05],\n",
       "        [0.06]],\n",
       "\n",
       "       [[0.03],\n",
       "        [0.04],\n",
       "        [0.05],\n",
       "        [0.06],\n",
       "        [0.07]],\n",
       "\n",
       "       [[0.04],\n",
       "        [0.05],\n",
       "        [0.06],\n",
       "        [0.07],\n",
       "        [0.08]],\n",
       "\n",
       "       [[0.05],\n",
       "        [0.06],\n",
       "        [0.07],\n",
       "        [0.08],\n",
       "        [0.09]],\n",
       "\n",
       "       [[0.06],\n",
       "        [0.07],\n",
       "        [0.08],\n",
       "        [0.09],\n",
       "        [0.1 ]],\n",
       "\n",
       "       [[0.07],\n",
       "        [0.08],\n",
       "        [0.09],\n",
       "        [0.1 ],\n",
       "        [0.11]],\n",
       "\n",
       "       [[0.08],\n",
       "        [0.09],\n",
       "        [0.1 ],\n",
       "        [0.11],\n",
       "        [0.12]],\n",
       "\n",
       "       [[0.09],\n",
       "        [0.1 ],\n",
       "        [0.11],\n",
       "        [0.12],\n",
       "        [0.13]],\n",
       "\n",
       "       [[0.1 ],\n",
       "        [0.11],\n",
       "        [0.12],\n",
       "        [0.13],\n",
       "        [0.14]],\n",
       "\n",
       "       [[0.11],\n",
       "        [0.12],\n",
       "        [0.13],\n",
       "        [0.14],\n",
       "        [0.15]],\n",
       "\n",
       "       [[0.12],\n",
       "        [0.13],\n",
       "        [0.14],\n",
       "        [0.15],\n",
       "        [0.16]],\n",
       "\n",
       "       [[0.13],\n",
       "        [0.14],\n",
       "        [0.15],\n",
       "        [0.16],\n",
       "        [0.17]],\n",
       "\n",
       "       [[0.14],\n",
       "        [0.15],\n",
       "        [0.16],\n",
       "        [0.17],\n",
       "        [0.18]],\n",
       "\n",
       "       [[0.15],\n",
       "        [0.16],\n",
       "        [0.17],\n",
       "        [0.18],\n",
       "        [0.19]],\n",
       "\n",
       "       [[0.16],\n",
       "        [0.17],\n",
       "        [0.18],\n",
       "        [0.19],\n",
       "        [0.2 ]],\n",
       "\n",
       "       [[0.17],\n",
       "        [0.18],\n",
       "        [0.19],\n",
       "        [0.2 ],\n",
       "        [0.21]],\n",
       "\n",
       "       [[0.18],\n",
       "        [0.19],\n",
       "        [0.2 ],\n",
       "        [0.21],\n",
       "        [0.22]],\n",
       "\n",
       "       [[0.19],\n",
       "        [0.2 ],\n",
       "        [0.21],\n",
       "        [0.22],\n",
       "        [0.23]],\n",
       "\n",
       "       [[0.2 ],\n",
       "        [0.21],\n",
       "        [0.22],\n",
       "        [0.23],\n",
       "        [0.24]],\n",
       "\n",
       "       [[0.21],\n",
       "        [0.22],\n",
       "        [0.23],\n",
       "        [0.24],\n",
       "        [0.25]],\n",
       "\n",
       "       [[0.22],\n",
       "        [0.23],\n",
       "        [0.24],\n",
       "        [0.25],\n",
       "        [0.26]],\n",
       "\n",
       "       [[0.23],\n",
       "        [0.24],\n",
       "        [0.25],\n",
       "        [0.26],\n",
       "        [0.27]],\n",
       "\n",
       "       [[0.24],\n",
       "        [0.25],\n",
       "        [0.26],\n",
       "        [0.27],\n",
       "        [0.28]],\n",
       "\n",
       "       [[0.25],\n",
       "        [0.26],\n",
       "        [0.27],\n",
       "        [0.28],\n",
       "        [0.29]],\n",
       "\n",
       "       [[0.26],\n",
       "        [0.27],\n",
       "        [0.28],\n",
       "        [0.29],\n",
       "        [0.3 ]],\n",
       "\n",
       "       [[0.27],\n",
       "        [0.28],\n",
       "        [0.29],\n",
       "        [0.3 ],\n",
       "        [0.31]],\n",
       "\n",
       "       [[0.28],\n",
       "        [0.29],\n",
       "        [0.3 ],\n",
       "        [0.31],\n",
       "        [0.32]],\n",
       "\n",
       "       [[0.29],\n",
       "        [0.3 ],\n",
       "        [0.31],\n",
       "        [0.32],\n",
       "        [0.33]],\n",
       "\n",
       "       [[0.3 ],\n",
       "        [0.31],\n",
       "        [0.32],\n",
       "        [0.33],\n",
       "        [0.34]],\n",
       "\n",
       "       [[0.31],\n",
       "        [0.32],\n",
       "        [0.33],\n",
       "        [0.34],\n",
       "        [0.35]],\n",
       "\n",
       "       [[0.32],\n",
       "        [0.33],\n",
       "        [0.34],\n",
       "        [0.35],\n",
       "        [0.36]],\n",
       "\n",
       "       [[0.33],\n",
       "        [0.34],\n",
       "        [0.35],\n",
       "        [0.36],\n",
       "        [0.37]],\n",
       "\n",
       "       [[0.34],\n",
       "        [0.35],\n",
       "        [0.36],\n",
       "        [0.37],\n",
       "        [0.38]],\n",
       "\n",
       "       [[0.35],\n",
       "        [0.36],\n",
       "        [0.37],\n",
       "        [0.38],\n",
       "        [0.39]],\n",
       "\n",
       "       [[0.36],\n",
       "        [0.37],\n",
       "        [0.38],\n",
       "        [0.39],\n",
       "        [0.4 ]],\n",
       "\n",
       "       [[0.37],\n",
       "        [0.38],\n",
       "        [0.39],\n",
       "        [0.4 ],\n",
       "        [0.41]],\n",
       "\n",
       "       [[0.38],\n",
       "        [0.39],\n",
       "        [0.4 ],\n",
       "        [0.41],\n",
       "        [0.42]],\n",
       "\n",
       "       [[0.39],\n",
       "        [0.4 ],\n",
       "        [0.41],\n",
       "        [0.42],\n",
       "        [0.43]],\n",
       "\n",
       "       [[0.4 ],\n",
       "        [0.41],\n",
       "        [0.42],\n",
       "        [0.43],\n",
       "        [0.44]],\n",
       "\n",
       "       [[0.41],\n",
       "        [0.42],\n",
       "        [0.43],\n",
       "        [0.44],\n",
       "        [0.45]],\n",
       "\n",
       "       [[0.42],\n",
       "        [0.43],\n",
       "        [0.44],\n",
       "        [0.45],\n",
       "        [0.46]],\n",
       "\n",
       "       [[0.43],\n",
       "        [0.44],\n",
       "        [0.45],\n",
       "        [0.46],\n",
       "        [0.47]],\n",
       "\n",
       "       [[0.44],\n",
       "        [0.45],\n",
       "        [0.46],\n",
       "        [0.47],\n",
       "        [0.48]],\n",
       "\n",
       "       [[0.45],\n",
       "        [0.46],\n",
       "        [0.47],\n",
       "        [0.48],\n",
       "        [0.49]],\n",
       "\n",
       "       [[0.46],\n",
       "        [0.47],\n",
       "        [0.48],\n",
       "        [0.49],\n",
       "        [0.5 ]],\n",
       "\n",
       "       [[0.47],\n",
       "        [0.48],\n",
       "        [0.49],\n",
       "        [0.5 ],\n",
       "        [0.51]],\n",
       "\n",
       "       [[0.48],\n",
       "        [0.49],\n",
       "        [0.5 ],\n",
       "        [0.51],\n",
       "        [0.52]],\n",
       "\n",
       "       [[0.49],\n",
       "        [0.5 ],\n",
       "        [0.51],\n",
       "        [0.52],\n",
       "        [0.53]],\n",
       "\n",
       "       [[0.5 ],\n",
       "        [0.51],\n",
       "        [0.52],\n",
       "        [0.53],\n",
       "        [0.54]],\n",
       "\n",
       "       [[0.51],\n",
       "        [0.52],\n",
       "        [0.53],\n",
       "        [0.54],\n",
       "        [0.55]],\n",
       "\n",
       "       [[0.52],\n",
       "        [0.53],\n",
       "        [0.54],\n",
       "        [0.55],\n",
       "        [0.56]],\n",
       "\n",
       "       [[0.53],\n",
       "        [0.54],\n",
       "        [0.55],\n",
       "        [0.56],\n",
       "        [0.57]],\n",
       "\n",
       "       [[0.54],\n",
       "        [0.55],\n",
       "        [0.56],\n",
       "        [0.57],\n",
       "        [0.58]],\n",
       "\n",
       "       [[0.55],\n",
       "        [0.56],\n",
       "        [0.57],\n",
       "        [0.58],\n",
       "        [0.59]],\n",
       "\n",
       "       [[0.56],\n",
       "        [0.57],\n",
       "        [0.58],\n",
       "        [0.59],\n",
       "        [0.6 ]],\n",
       "\n",
       "       [[0.57],\n",
       "        [0.58],\n",
       "        [0.59],\n",
       "        [0.6 ],\n",
       "        [0.61]],\n",
       "\n",
       "       [[0.58],\n",
       "        [0.59],\n",
       "        [0.6 ],\n",
       "        [0.61],\n",
       "        [0.62]],\n",
       "\n",
       "       [[0.59],\n",
       "        [0.6 ],\n",
       "        [0.61],\n",
       "        [0.62],\n",
       "        [0.63]],\n",
       "\n",
       "       [[0.6 ],\n",
       "        [0.61],\n",
       "        [0.62],\n",
       "        [0.63],\n",
       "        [0.64]],\n",
       "\n",
       "       [[0.61],\n",
       "        [0.62],\n",
       "        [0.63],\n",
       "        [0.64],\n",
       "        [0.65]],\n",
       "\n",
       "       [[0.62],\n",
       "        [0.63],\n",
       "        [0.64],\n",
       "        [0.65],\n",
       "        [0.66]],\n",
       "\n",
       "       [[0.63],\n",
       "        [0.64],\n",
       "        [0.65],\n",
       "        [0.66],\n",
       "        [0.67]],\n",
       "\n",
       "       [[0.64],\n",
       "        [0.65],\n",
       "        [0.66],\n",
       "        [0.67],\n",
       "        [0.68]],\n",
       "\n",
       "       [[0.65],\n",
       "        [0.66],\n",
       "        [0.67],\n",
       "        [0.68],\n",
       "        [0.69]],\n",
       "\n",
       "       [[0.66],\n",
       "        [0.67],\n",
       "        [0.68],\n",
       "        [0.69],\n",
       "        [0.7 ]],\n",
       "\n",
       "       [[0.67],\n",
       "        [0.68],\n",
       "        [0.69],\n",
       "        [0.7 ],\n",
       "        [0.71]],\n",
       "\n",
       "       [[0.68],\n",
       "        [0.69],\n",
       "        [0.7 ],\n",
       "        [0.71],\n",
       "        [0.72]],\n",
       "\n",
       "       [[0.69],\n",
       "        [0.7 ],\n",
       "        [0.71],\n",
       "        [0.72],\n",
       "        [0.73]],\n",
       "\n",
       "       [[0.7 ],\n",
       "        [0.71],\n",
       "        [0.72],\n",
       "        [0.73],\n",
       "        [0.74]],\n",
       "\n",
       "       [[0.71],\n",
       "        [0.72],\n",
       "        [0.73],\n",
       "        [0.74],\n",
       "        [0.75]],\n",
       "\n",
       "       [[0.72],\n",
       "        [0.73],\n",
       "        [0.74],\n",
       "        [0.75],\n",
       "        [0.76]],\n",
       "\n",
       "       [[0.73],\n",
       "        [0.74],\n",
       "        [0.75],\n",
       "        [0.76],\n",
       "        [0.77]],\n",
       "\n",
       "       [[0.74],\n",
       "        [0.75],\n",
       "        [0.76],\n",
       "        [0.77],\n",
       "        [0.78]],\n",
       "\n",
       "       [[0.75],\n",
       "        [0.76],\n",
       "        [0.77],\n",
       "        [0.78],\n",
       "        [0.79]],\n",
       "\n",
       "       [[0.76],\n",
       "        [0.77],\n",
       "        [0.78],\n",
       "        [0.79],\n",
       "        [0.8 ]],\n",
       "\n",
       "       [[0.77],\n",
       "        [0.78],\n",
       "        [0.79],\n",
       "        [0.8 ],\n",
       "        [0.81]],\n",
       "\n",
       "       [[0.78],\n",
       "        [0.79],\n",
       "        [0.8 ],\n",
       "        [0.81],\n",
       "        [0.82]],\n",
       "\n",
       "       [[0.79],\n",
       "        [0.8 ],\n",
       "        [0.81],\n",
       "        [0.82],\n",
       "        [0.83]],\n",
       "\n",
       "       [[0.8 ],\n",
       "        [0.81],\n",
       "        [0.82],\n",
       "        [0.83],\n",
       "        [0.84]],\n",
       "\n",
       "       [[0.81],\n",
       "        [0.82],\n",
       "        [0.83],\n",
       "        [0.84],\n",
       "        [0.85]],\n",
       "\n",
       "       [[0.82],\n",
       "        [0.83],\n",
       "        [0.84],\n",
       "        [0.85],\n",
       "        [0.86]],\n",
       "\n",
       "       [[0.83],\n",
       "        [0.84],\n",
       "        [0.85],\n",
       "        [0.86],\n",
       "        [0.87]],\n",
       "\n",
       "       [[0.84],\n",
       "        [0.85],\n",
       "        [0.86],\n",
       "        [0.87],\n",
       "        [0.88]],\n",
       "\n",
       "       [[0.85],\n",
       "        [0.86],\n",
       "        [0.87],\n",
       "        [0.88],\n",
       "        [0.89]],\n",
       "\n",
       "       [[0.86],\n",
       "        [0.87],\n",
       "        [0.88],\n",
       "        [0.89],\n",
       "        [0.9 ]],\n",
       "\n",
       "       [[0.87],\n",
       "        [0.88],\n",
       "        [0.89],\n",
       "        [0.9 ],\n",
       "        [0.91]],\n",
       "\n",
       "       [[0.88],\n",
       "        [0.89],\n",
       "        [0.9 ],\n",
       "        [0.91],\n",
       "        [0.92]],\n",
       "\n",
       "       [[0.89],\n",
       "        [0.9 ],\n",
       "        [0.91],\n",
       "        [0.92],\n",
       "        [0.93]],\n",
       "\n",
       "       [[0.9 ],\n",
       "        [0.91],\n",
       "        [0.92],\n",
       "        [0.93],\n",
       "        [0.94]],\n",
       "\n",
       "       [[0.91],\n",
       "        [0.92],\n",
       "        [0.93],\n",
       "        [0.94],\n",
       "        [0.95]],\n",
       "\n",
       "       [[0.92],\n",
       "        [0.93],\n",
       "        [0.94],\n",
       "        [0.95],\n",
       "        [0.96]],\n",
       "\n",
       "       [[0.93],\n",
       "        [0.94],\n",
       "        [0.95],\n",
       "        [0.96],\n",
       "        [0.97]],\n",
       "\n",
       "       [[0.94],\n",
       "        [0.95],\n",
       "        [0.96],\n",
       "        [0.97],\n",
       "        [0.98]],\n",
       "\n",
       "       [[0.95],\n",
       "        [0.96],\n",
       "        [0.97],\n",
       "        [0.98],\n",
       "        [0.99]],\n",
       "\n",
       "       [[0.96],\n",
       "        [0.97],\n",
       "        [0.98],\n",
       "        [0.99],\n",
       "        [1.  ]],\n",
       "\n",
       "       [[0.97],\n",
       "        [0.98],\n",
       "        [0.99],\n",
       "        [1.  ],\n",
       "        [1.01]],\n",
       "\n",
       "       [[0.98],\n",
       "        [0.99],\n",
       "        [1.  ],\n",
       "        [1.01],\n",
       "        [1.02]],\n",
       "\n",
       "       [[0.99],\n",
       "        [1.  ],\n",
       "        [1.01],\n",
       "        [1.02],\n",
       "        [1.03]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = [[[(i+j)/100] for i in range (5)] for j in range (100)]\n",
    "\n",
    "'''\n",
    "[[0], [1], [2], [3], [4]],\n",
    " [[1], [2], [3], [4], [5]],\n",
    " [[2], [3], [4], [5], [6]],\n",
    " [[3], [4], [5], [6], [7]],\n",
    " [[4], [5], [6], [7], [8]],\n",
    " [[5], [6], [7], [8], [9]],\n",
    " [[6], [7], [8], [9], [10]],....\n",
    "\n",
    "'''\n",
    "\n",
    "data = np.array(Data, dtype=float)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3242,
     "status": "ok",
     "timestamp": 1562898065395,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "n-qn_634fvSC",
    "outputId": "500bfbd6-3ae7-486e-96de-35909cdb7bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target = [(i+5)/100 for i in range(100)]\n",
    "'''\n",
    "[0.05,\n",
    " 0.06,\n",
    " 0.07,\n",
    " 0.08,\n",
    " 0.09,\n",
    " 0.1,\n",
    " 0.11,....\n",
    "\n",
    "'''\n",
    "target = np.array(Target, dtype=float)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYrW8DyEgvx2"
   },
   "outputs": [],
   "source": [
    "#                                  train_test_split(*arrays, **options)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99JQZ5XLiw5f"
   },
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1562898065642,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "kNs5I_VAjAAk",
    "outputId": "3cfff8af-b286-45cf-f24e-3e61d73d6137"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 02:21:04.671928 140167825508224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0712 02:21:04.717792 140167825508224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0712 02:21:04.724888 140167825508224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "#      use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "#      bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "#      recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "#      kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
    "#      recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, \n",
    "#      go_backwards=False, stateful=False, unroll=False, **kwargs)\n",
    "\n",
    "\n",
    "model.add(LSTM((1), batch_input_shape = (None,None,1), return_sequences = True))\n",
    "model.add(LSTM((1), return_sequences = False))\n",
    "\n",
    "#model.add(LSTM((1), activation='sigmoid', dropout=0.3, recurrent_dropout=0.2, return_sequences=False))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1562898065644,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "UsQKMNxHk0KM",
    "outputId": "ae8f5dbe-d5bc-43d0-c7c8-726c71336282"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 02:21:05.171054 140167825508224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer = 'Adadelta', metrics = ['accuracy'])\n",
    "\n",
    "#model.compile(loss='mean_absolute_error', optimizer = 'Adadelta', metrics = ['accuracy'])\n",
    "#model.compile(loss='mean_absolute_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#model.compile(loss='mean_absolute_error', optimizer = 'RMSprop', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1562898068399,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "op2ojEN-k0Mw",
    "outputId": "34fd8455-91ac-490c-df1d-1b37664f4b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 1)           12        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31132,
     "status": "ok",
     "timestamp": 1562898494031,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "XMskN5SDk0Ph",
    "outputId": "d44fc462-2634-429a-cadb-a3cccc4083fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 8.2817e-04 - acc: 0.0133 - val_loss: 7.8710e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/382\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 8.2708e-04 - acc: 0.0133 - val_loss: 7.7737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 8.2377e-04 - acc: 0.0133 - val_loss: 7.8889e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 8.3566e-04 - acc: 0.0133 - val_loss: 7.6950e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 8.2923e-04 - acc: 0.0133 - val_loss: 7.9042e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/382\n",
      "75/75 [==============================] - 0s 974us/step - loss: 8.3067e-04 - acc: 0.0133 - val_loss: 7.7404e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/382\n",
      "75/75 [==============================] - 0s 998us/step - loss: 8.1924e-04 - acc: 0.0133 - val_loss: 7.8296e-04 - val_acc: 0.0000e+00\n",
      "Epoch 8/382\n",
      "75/75 [==============================] - 0s 973us/step - loss: 8.2235e-04 - acc: 0.0133 - val_loss: 7.8836e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/382\n",
      "75/75 [==============================] - 0s 918us/step - loss: 8.1005e-04 - acc: 0.0133 - val_loss: 7.6901e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/382\n",
      "75/75 [==============================] - 0s 937us/step - loss: 8.0168e-04 - acc: 0.0133 - val_loss: 7.6063e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/382\n",
      "75/75 [==============================] - 0s 890us/step - loss: 8.0058e-04 - acc: 0.0133 - val_loss: 7.5796e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/382\n",
      "75/75 [==============================] - 0s 908us/step - loss: 7.9331e-04 - acc: 0.0133 - val_loss: 7.6158e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/382\n",
      "75/75 [==============================] - 0s 927us/step - loss: 7.9438e-04 - acc: 0.0133 - val_loss: 7.5937e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/382\n",
      "75/75 [==============================] - 0s 906us/step - loss: 7.8365e-04 - acc: 0.0133 - val_loss: 7.6389e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/382\n",
      "75/75 [==============================] - 0s 986us/step - loss: 7.9091e-04 - acc: 0.0133 - val_loss: 7.6291e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/382\n",
      "75/75 [==============================] - 0s 948us/step - loss: 7.8479e-04 - acc: 0.0133 - val_loss: 7.6325e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/382\n",
      "75/75 [==============================] - 0s 911us/step - loss: 7.8058e-04 - acc: 0.0133 - val_loss: 7.7095e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/382\n",
      "75/75 [==============================] - 0s 908us/step - loss: 7.9510e-04 - acc: 0.0133 - val_loss: 7.7171e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/382\n",
      "75/75 [==============================] - 0s 896us/step - loss: 7.8627e-04 - acc: 0.0133 - val_loss: 7.6041e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/382\n",
      "75/75 [==============================] - 0s 913us/step - loss: 7.8129e-04 - acc: 0.0133 - val_loss: 7.5651e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/382\n",
      "75/75 [==============================] - 0s 949us/step - loss: 7.8298e-04 - acc: 0.0133 - val_loss: 7.5808e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/382\n",
      "75/75 [==============================] - 0s 896us/step - loss: 7.7384e-04 - acc: 0.0133 - val_loss: 7.5496e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/382\n",
      "75/75 [==============================] - 0s 941us/step - loss: 7.7155e-04 - acc: 0.0133 - val_loss: 7.5771e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/382\n",
      "75/75 [==============================] - 0s 908us/step - loss: 7.7067e-04 - acc: 0.0133 - val_loss: 7.6380e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/382\n",
      "75/75 [==============================] - 0s 950us/step - loss: 7.7711e-04 - acc: 0.0133 - val_loss: 7.5260e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/382\n",
      "75/75 [==============================] - 0s 917us/step - loss: 7.6385e-04 - acc: 0.0133 - val_loss: 7.4913e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/382\n",
      "75/75 [==============================] - 0s 925us/step - loss: 7.6985e-04 - acc: 0.0133 - val_loss: 7.5019e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/382\n",
      "75/75 [==============================] - 0s 880us/step - loss: 7.6436e-04 - acc: 0.0133 - val_loss: 7.5440e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.7073e-04 - acc: 0.0133 - val_loss: 7.5024e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/382\n",
      "75/75 [==============================] - 0s 914us/step - loss: 7.6421e-04 - acc: 0.0133 - val_loss: 7.4502e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/382\n",
      "75/75 [==============================] - 0s 963us/step - loss: 7.6027e-04 - acc: 0.0133 - val_loss: 7.6119e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/382\n",
      "75/75 [==============================] - 0s 983us/step - loss: 7.6637e-04 - acc: 0.0133 - val_loss: 7.4053e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/382\n",
      "75/75 [==============================] - 0s 982us/step - loss: 7.5595e-04 - acc: 0.0133 - val_loss: 7.4242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/382\n",
      "75/75 [==============================] - 0s 989us/step - loss: 7.5975e-04 - acc: 0.0133 - val_loss: 7.3951e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/382\n",
      "75/75 [==============================] - 0s 974us/step - loss: 7.5517e-04 - acc: 0.0133 - val_loss: 7.4453e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/382\n",
      "75/75 [==============================] - 0s 967us/step - loss: 7.7686e-04 - acc: 0.0133 - val_loss: 7.4020e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.6103e-04 - acc: 0.0133 - val_loss: 7.4317e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.5913e-04 - acc: 0.0133 - val_loss: 7.3147e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/382\n",
      "75/75 [==============================] - 0s 988us/step - loss: 7.5073e-04 - acc: 0.0133 - val_loss: 7.3388e-04 - val_acc: 0.0000e+00\n",
      "Epoch 40/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 7.5936e-04 - acc: 0.0133 - val_loss: 7.2739e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/382\n",
      "75/75 [==============================] - 0s 960us/step - loss: 7.5407e-04 - acc: 0.0133 - val_loss: 7.5178e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.7216e-04 - acc: 0.0133 - val_loss: 7.2483e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.5772e-04 - acc: 0.0133 - val_loss: 7.3129e-04 - val_acc: 0.0000e+00\n",
      "Epoch 44/382\n",
      "75/75 [==============================] - 0s 997us/step - loss: 7.4455e-04 - acc: 0.0133 - val_loss: 7.2326e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/382\n",
      "75/75 [==============================] - 0s 981us/step - loss: 7.4299e-04 - acc: 0.0133 - val_loss: 7.2503e-04 - val_acc: 0.0000e+00\n",
      "Epoch 46/382\n",
      "75/75 [==============================] - 0s 978us/step - loss: 7.4973e-04 - acc: 0.0133 - val_loss: 7.3338e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.4381e-04 - acc: 0.0133 - val_loss: 7.2742e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.5886e-04 - acc: 0.0133 - val_loss: 7.2565e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/382\n",
      "75/75 [==============================] - 0s 992us/step - loss: 7.4310e-04 - acc: 0.0133 - val_loss: 7.3695e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/382\n",
      "75/75 [==============================] - 0s 998us/step - loss: 7.4528e-04 - acc: 0.0133 - val_loss: 7.1425e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.5100e-04 - acc: 0.0133 - val_loss: 7.1192e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.3627e-04 - acc: 0.0133 - val_loss: 7.0107e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/382\n",
      "75/75 [==============================] - 0s 996us/step - loss: 7.4212e-04 - acc: 0.0133 - val_loss: 7.2324e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 7.5046e-04 - acc: 0.0133 - val_loss: 7.0240e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 7.5183e-04 - acc: 0.0133 - val_loss: 7.0320e-04 - val_acc: 0.0000e+00\n",
      "Epoch 56/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.3218e-04 - acc: 0.0133 - val_loss: 7.2211e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/382\n",
      "75/75 [==============================] - 0s 981us/step - loss: 7.3715e-04 - acc: 0.0133 - val_loss: 6.9616e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/382\n",
      "75/75 [==============================] - 0s 967us/step - loss: 7.3121e-04 - acc: 0.0133 - val_loss: 6.9891e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/382\n",
      "75/75 [==============================] - 0s 975us/step - loss: 7.3013e-04 - acc: 0.0133 - val_loss: 6.9397e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/382\n",
      "75/75 [==============================] - 0s 997us/step - loss: 7.3321e-04 - acc: 0.0133 - val_loss: 6.9471e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/382\n",
      "75/75 [==============================] - 0s 984us/step - loss: 7.2863e-04 - acc: 0.0133 - val_loss: 7.0142e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 7.3538e-04 - acc: 0.0133 - val_loss: 6.9976e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/382\n",
      "75/75 [==============================] - 0s 997us/step - loss: 7.2229e-04 - acc: 0.0133 - val_loss: 6.9302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2121e-04 - acc: 0.0133 - val_loss: 6.9588e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 7.2152e-04 - acc: 0.0133 - val_loss: 6.9319e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/382\n",
      "75/75 [==============================] - 0s 950us/step - loss: 7.2249e-04 - acc: 0.0133 - val_loss: 6.9476e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.1815e-04 - acc: 0.0133 - val_loss: 6.9260e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2309e-04 - acc: 0.0133 - val_loss: 7.1310e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2366e-04 - acc: 0.0133 - val_loss: 6.9166e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2717e-04 - acc: 0.0133 - val_loss: 7.2321e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.3359e-04 - acc: 0.0133 - val_loss: 6.9647e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2769e-04 - acc: 0.0133 - val_loss: 6.8097e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.1513e-04 - acc: 0.0133 - val_loss: 6.9088e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/382\n",
      "75/75 [==============================] - 0s 975us/step - loss: 7.2236e-04 - acc: 0.0133 - val_loss: 6.8283e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/382\n",
      "75/75 [==============================] - 0s 975us/step - loss: 7.1743e-04 - acc: 0.0133 - val_loss: 6.8101e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2529e-04 - acc: 0.0133 - val_loss: 6.7814e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/382\n",
      "75/75 [==============================] - 0s 969us/step - loss: 7.0770e-04 - acc: 0.0133 - val_loss: 6.7550e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/382\n",
      "75/75 [==============================] - 0s 997us/step - loss: 7.1184e-04 - acc: 0.0133 - val_loss: 6.7755e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.1613e-04 - acc: 0.0133 - val_loss: 6.7778e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2694e-04 - acc: 0.0133 - val_loss: 6.7680e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.1298e-04 - acc: 0.0133 - val_loss: 6.5965e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.1661e-04 - acc: 0.0133 - val_loss: 6.6164e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/382\n",
      "75/75 [==============================] - 0s 973us/step - loss: 7.1108e-04 - acc: 0.0133 - val_loss: 6.5907e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0386e-04 - acc: 0.0133 - val_loss: 6.6105e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0645e-04 - acc: 0.0133 - val_loss: 6.6161e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0498e-04 - acc: 0.0133 - val_loss: 6.6289e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/382\n",
      "75/75 [==============================] - 0s 984us/step - loss: 6.9903e-04 - acc: 0.0133 - val_loss: 6.5481e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/382\n",
      "75/75 [==============================] - 0s 997us/step - loss: 6.9801e-04 - acc: 0.0133 - val_loss: 6.5529e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0893e-04 - acc: 0.0133 - val_loss: 6.5129e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/382\n",
      "75/75 [==============================] - 0s 991us/step - loss: 6.9748e-04 - acc: 0.0133 - val_loss: 6.5288e-04 - val_acc: 0.0000e+00\n",
      "Epoch 91/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0320e-04 - acc: 0.0133 - val_loss: 6.5691e-04 - val_acc: 0.0000e+00\n",
      "Epoch 92/382\n",
      "75/75 [==============================] - 0s 959us/step - loss: 6.9458e-04 - acc: 0.0133 - val_loss: 6.5281e-04 - val_acc: 0.0000e+00\n",
      "Epoch 93/382\n",
      "75/75 [==============================] - 0s 962us/step - loss: 7.0033e-04 - acc: 0.0133 - val_loss: 6.5232e-04 - val_acc: 0.0000e+00\n",
      "Epoch 94/382\n",
      "75/75 [==============================] - 0s 963us/step - loss: 6.9406e-04 - acc: 0.0133 - val_loss: 6.5969e-04 - val_acc: 0.0000e+00\n",
      "Epoch 95/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.2324e-04 - acc: 0.0133 - val_loss: 6.7309e-04 - val_acc: 0.0000e+00\n",
      "Epoch 96/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0406e-04 - acc: 0.0133 - val_loss: 6.7405e-04 - val_acc: 0.0000e+00\n",
      "Epoch 97/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 6.9950e-04 - acc: 0.0133 - val_loss: 6.5784e-04 - val_acc: 0.0000e+00\n",
      "Epoch 98/382\n",
      "75/75 [==============================] - 0s 984us/step - loss: 6.9630e-04 - acc: 0.0133 - val_loss: 6.6265e-04 - val_acc: 0.0000e+00\n",
      "Epoch 99/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.9783e-04 - acc: 0.0133 - val_loss: 6.6289e-04 - val_acc: 0.0000e+00\n",
      "Epoch 100/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.9761e-04 - acc: 0.0133 - val_loss: 6.4312e-04 - val_acc: 0.0000e+00\n",
      "Epoch 101/382\n",
      "75/75 [==============================] - 0s 994us/step - loss: 6.9409e-04 - acc: 0.0133 - val_loss: 6.4378e-04 - val_acc: 0.0000e+00\n",
      "Epoch 102/382\n",
      "75/75 [==============================] - 0s 998us/step - loss: 6.8899e-04 - acc: 0.0133 - val_loss: 6.4048e-04 - val_acc: 0.0000e+00\n",
      "Epoch 103/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8638e-04 - acc: 0.0133 - val_loss: 6.5162e-04 - val_acc: 0.0000e+00\n",
      "Epoch 104/382\n",
      "75/75 [==============================] - 0s 956us/step - loss: 6.9529e-04 - acc: 0.0133 - val_loss: 6.3223e-04 - val_acc: 0.0000e+00\n",
      "Epoch 105/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8569e-04 - acc: 0.0133 - val_loss: 6.2895e-04 - val_acc: 0.0000e+00\n",
      "Epoch 106/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8504e-04 - acc: 0.0133 - val_loss: 6.2887e-04 - val_acc: 0.0000e+00\n",
      "Epoch 107/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8294e-04 - acc: 0.0133 - val_loss: 6.3478e-04 - val_acc: 0.0000e+00\n",
      "Epoch 108/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7852e-04 - acc: 0.0133 - val_loss: 6.3144e-04 - val_acc: 0.0000e+00\n",
      "Epoch 109/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8132e-04 - acc: 0.0133 - val_loss: 6.3664e-04 - val_acc: 0.0000e+00\n",
      "Epoch 110/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8746e-04 - acc: 0.0133 - val_loss: 6.2824e-04 - val_acc: 0.0000e+00\n",
      "Epoch 111/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8911e-04 - acc: 0.0133 - val_loss: 6.2769e-04 - val_acc: 0.0000e+00\n",
      "Epoch 112/382\n",
      "75/75 [==============================] - 0s 978us/step - loss: 6.8267e-04 - acc: 0.0133 - val_loss: 6.1602e-04 - val_acc: 0.0000e+00\n",
      "Epoch 113/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8887e-04 - acc: 0.0133 - val_loss: 6.1980e-04 - val_acc: 0.0000e+00\n",
      "Epoch 114/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.9250e-04 - acc: 0.0133 - val_loss: 6.3042e-04 - val_acc: 0.0000e+00\n",
      "Epoch 115/382\n",
      "75/75 [==============================] - 0s 983us/step - loss: 6.9119e-04 - acc: 0.0133 - val_loss: 6.4128e-04 - val_acc: 0.0000e+00\n",
      "Epoch 116/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8511e-04 - acc: 0.0133 - val_loss: 6.2051e-04 - val_acc: 0.0000e+00\n",
      "Epoch 117/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7217e-04 - acc: 0.0133 - val_loss: 6.2154e-04 - val_acc: 0.0000e+00\n",
      "Epoch 118/382\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 6.7523e-04 - acc: 0.0133 - val_loss: 6.1722e-04 - val_acc: 0.0000e+00\n",
      "Epoch 119/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0081e-04 - acc: 0.0133 - val_loss: 6.2206e-04 - val_acc: 0.0000e+00\n",
      "Epoch 120/382\n",
      "75/75 [==============================] - 0s 991us/step - loss: 6.8219e-04 - acc: 0.0133 - val_loss: 6.2167e-04 - val_acc: 0.0000e+00\n",
      "Epoch 121/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7799e-04 - acc: 0.0133 - val_loss: 6.2135e-04 - val_acc: 0.0000e+00\n",
      "Epoch 122/382\n",
      "75/75 [==============================] - 0s 971us/step - loss: 6.7645e-04 - acc: 0.0133 - val_loss: 6.2272e-04 - val_acc: 0.0000e+00\n",
      "Epoch 123/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7501e-04 - acc: 0.0133 - val_loss: 6.1835e-04 - val_acc: 0.0000e+00\n",
      "Epoch 124/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8278e-04 - acc: 0.0133 - val_loss: 6.1320e-04 - val_acc: 0.0000e+00\n",
      "Epoch 125/382\n",
      "75/75 [==============================] - 0s 967us/step - loss: 6.7406e-04 - acc: 0.0133 - val_loss: 6.1022e-04 - val_acc: 0.0000e+00\n",
      "Epoch 126/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7563e-04 - acc: 0.0133 - val_loss: 6.1113e-04 - val_acc: 0.0000e+00\n",
      "Epoch 127/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7483e-04 - acc: 0.0133 - val_loss: 6.0565e-04 - val_acc: 0.0000e+00\n",
      "Epoch 128/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7319e-04 - acc: 0.0133 - val_loss: 6.1183e-04 - val_acc: 0.0000e+00\n",
      "Epoch 129/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8595e-04 - acc: 0.0133 - val_loss: 6.1109e-04 - val_acc: 0.0000e+00\n",
      "Epoch 130/382\n",
      "75/75 [==============================] - 0s 989us/step - loss: 6.6768e-04 - acc: 0.0133 - val_loss: 6.0938e-04 - val_acc: 0.0000e+00\n",
      "Epoch 131/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7911e-04 - acc: 0.0133 - val_loss: 6.1632e-04 - val_acc: 0.0000e+00\n",
      "Epoch 132/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6982e-04 - acc: 0.0133 - val_loss: 6.1243e-04 - val_acc: 0.0000e+00\n",
      "Epoch 133/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 6.7090e-04 - acc: 0.0133 - val_loss: 6.1123e-04 - val_acc: 0.0000e+00\n",
      "Epoch 134/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6277e-04 - acc: 0.0133 - val_loss: 6.1574e-04 - val_acc: 0.0000e+00\n",
      "Epoch 135/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 6.6217e-04 - acc: 0.0133 - val_loss: 6.1093e-04 - val_acc: 0.0000e+00\n",
      "Epoch 136/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6125e-04 - acc: 0.0133 - val_loss: 6.0725e-04 - val_acc: 0.0000e+00\n",
      "Epoch 137/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6203e-04 - acc: 0.0133 - val_loss: 6.0336e-04 - val_acc: 0.0000e+00\n",
      "Epoch 138/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5709e-04 - acc: 0.0133 - val_loss: 6.1197e-04 - val_acc: 0.0000e+00\n",
      "Epoch 139/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6018e-04 - acc: 0.0133 - val_loss: 6.0596e-04 - val_acc: 0.0000e+00\n",
      "Epoch 140/382\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 6.5967e-04 - acc: 0.0133 - val_loss: 6.0415e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/382\n",
      "75/75 [==============================] - 0s 975us/step - loss: 6.5975e-04 - acc: 0.0133 - val_loss: 6.1805e-04 - val_acc: 0.0000e+00\n",
      "Epoch 142/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 6.5992e-04 - acc: 0.0133 - val_loss: 5.9018e-04 - val_acc: 0.0000e+00\n",
      "Epoch 143/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5545e-04 - acc: 0.0133 - val_loss: 6.2102e-04 - val_acc: 0.0000e+00\n",
      "Epoch 144/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6355e-04 - acc: 0.0133 - val_loss: 5.8971e-04 - val_acc: 0.0000e+00\n",
      "Epoch 145/382\n",
      "75/75 [==============================] - 0s 969us/step - loss: 6.5291e-04 - acc: 0.0133 - val_loss: 5.9991e-04 - val_acc: 0.0000e+00\n",
      "Epoch 146/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5334e-04 - acc: 0.0133 - val_loss: 5.9034e-04 - val_acc: 0.0000e+00\n",
      "Epoch 147/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6460e-04 - acc: 0.0133 - val_loss: 6.2343e-04 - val_acc: 0.0000e+00\n",
      "Epoch 148/382\n",
      "75/75 [==============================] - 0s 991us/step - loss: 6.8735e-04 - acc: 0.0133 - val_loss: 5.8230e-04 - val_acc: 0.0000e+00\n",
      "Epoch 149/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 6.4879e-04 - acc: 0.0133 - val_loss: 5.7947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 150/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5117e-04 - acc: 0.0133 - val_loss: 5.7948e-04 - val_acc: 0.0000e+00\n",
      "Epoch 151/382\n",
      "75/75 [==============================] - 0s 991us/step - loss: 6.5727e-04 - acc: 0.0133 - val_loss: 5.8279e-04 - val_acc: 0.0000e+00\n",
      "Epoch 152/382\n",
      "75/75 [==============================] - 0s 988us/step - loss: 6.5500e-04 - acc: 0.0133 - val_loss: 5.8373e-04 - val_acc: 0.0000e+00\n",
      "Epoch 153/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5335e-04 - acc: 0.0133 - val_loss: 5.8116e-04 - val_acc: 0.0000e+00\n",
      "Epoch 154/382\n",
      "75/75 [==============================] - 0s 992us/step - loss: 6.4690e-04 - acc: 0.0133 - val_loss: 5.8382e-04 - val_acc: 0.0000e+00\n",
      "Epoch 155/382\n",
      "75/75 [==============================] - 0s 949us/step - loss: 6.4796e-04 - acc: 0.0133 - val_loss: 5.8514e-04 - val_acc: 0.0000e+00\n",
      "Epoch 156/382\n",
      "75/75 [==============================] - 0s 959us/step - loss: 6.4698e-04 - acc: 0.0133 - val_loss: 5.9552e-04 - val_acc: 0.0000e+00\n",
      "Epoch 157/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 6.4965e-04 - acc: 0.0133 - val_loss: 5.8592e-04 - val_acc: 0.0000e+00\n",
      "Epoch 158/382\n",
      "75/75 [==============================] - 0s 981us/step - loss: 6.4519e-04 - acc: 0.0133 - val_loss: 5.7564e-04 - val_acc: 0.0000e+00\n",
      "Epoch 159/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.6128e-04 - acc: 0.0133 - val_loss: 5.6924e-04 - val_acc: 0.0000e+00\n",
      "Epoch 160/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4548e-04 - acc: 0.0133 - val_loss: 5.7707e-04 - val_acc: 0.0000e+00\n",
      "Epoch 161/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 6.4722e-04 - acc: 0.0133 - val_loss: 5.7162e-04 - val_acc: 0.0000e+00\n",
      "Epoch 162/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4230e-04 - acc: 0.0133 - val_loss: 5.7760e-04 - val_acc: 0.0000e+00\n",
      "Epoch 163/382\n",
      "75/75 [==============================] - 0s 979us/step - loss: 6.6408e-04 - acc: 0.0133 - val_loss: 5.6985e-04 - val_acc: 0.0000e+00\n",
      "Epoch 164/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4641e-04 - acc: 0.0133 - val_loss: 5.6992e-04 - val_acc: 0.0000e+00\n",
      "Epoch 165/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5778e-04 - acc: 0.0133 - val_loss: 5.8999e-04 - val_acc: 0.0000e+00\n",
      "Epoch 166/382\n",
      "75/75 [==============================] - 0s 988us/step - loss: 6.4694e-04 - acc: 0.0133 - val_loss: 5.6570e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5275e-04 - acc: 0.0133 - val_loss: 5.7760e-04 - val_acc: 0.0000e+00\n",
      "Epoch 168/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4743e-04 - acc: 0.0133 - val_loss: 5.7082e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/382\n",
      "75/75 [==============================] - 0s 980us/step - loss: 6.4235e-04 - acc: 0.0133 - val_loss: 5.6511e-04 - val_acc: 0.0000e+00\n",
      "Epoch 170/382\n",
      "75/75 [==============================] - 0s 986us/step - loss: 6.4214e-04 - acc: 0.0133 - val_loss: 5.6374e-04 - val_acc: 0.0000e+00\n",
      "Epoch 171/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4159e-04 - acc: 0.0133 - val_loss: 5.6642e-04 - val_acc: 0.0000e+00\n",
      "Epoch 172/382\n",
      "75/75 [==============================] - 0s 956us/step - loss: 6.5240e-04 - acc: 0.0133 - val_loss: 5.7237e-04 - val_acc: 0.0000e+00\n",
      "Epoch 173/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4330e-04 - acc: 0.0133 - val_loss: 5.6401e-04 - val_acc: 0.0000e+00\n",
      "Epoch 174/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.3799e-04 - acc: 0.0133 - val_loss: 5.5581e-04 - val_acc: 0.0000e+00\n",
      "Epoch 175/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.3416e-04 - acc: 0.0133 - val_loss: 5.5310e-04 - val_acc: 0.0000e+00\n",
      "Epoch 176/382\n",
      "75/75 [==============================] - 0s 959us/step - loss: 6.3594e-04 - acc: 0.0133 - val_loss: 5.4966e-04 - val_acc: 0.0000e+00\n",
      "Epoch 177/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.3611e-04 - acc: 0.0133 - val_loss: 5.5371e-04 - val_acc: 0.0000e+00\n",
      "Epoch 178/382\n",
      "75/75 [==============================] - 0s 989us/step - loss: 6.3355e-04 - acc: 0.0133 - val_loss: 5.5213e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 6.4029e-04 - acc: 0.0133 - val_loss: 5.6585e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.3851e-04 - acc: 0.0133 - val_loss: 5.5242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 181/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 6.3406e-04 - acc: 0.0133 - val_loss: 5.5042e-04 - val_acc: 0.0000e+00\n",
      "Epoch 182/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 6.4684e-04 - acc: 0.0133 - val_loss: 5.6264e-04 - val_acc: 0.0000e+00\n",
      "Epoch 183/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.3848e-04 - acc: 0.0133 - val_loss: 5.7198e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 6.3730e-04 - acc: 0.0133 - val_loss: 5.4831e-04 - val_acc: 0.0000e+00\n",
      "Epoch 185/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2912e-04 - acc: 0.0133 - val_loss: 5.4805e-04 - val_acc: 0.0000e+00\n",
      "Epoch 186/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2808e-04 - acc: 0.0133 - val_loss: 5.4572e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2809e-04 - acc: 0.0133 - val_loss: 5.4488e-04 - val_acc: 0.0000e+00\n",
      "Epoch 188/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2829e-04 - acc: 0.0133 - val_loss: 5.6829e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 6.3847e-04 - acc: 0.0133 - val_loss: 5.4630e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2813e-04 - acc: 0.0133 - val_loss: 5.5103e-04 - val_acc: 0.0000e+00\n",
      "Epoch 191/382\n",
      "75/75 [==============================] - 0s 995us/step - loss: 6.2508e-04 - acc: 0.0133 - val_loss: 5.4730e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4014e-04 - acc: 0.0133 - val_loss: 5.4152e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/382\n",
      "75/75 [==============================] - 0s 973us/step - loss: 6.3393e-04 - acc: 0.0133 - val_loss: 5.5691e-04 - val_acc: 0.0000e+00\n",
      "Epoch 194/382\n",
      "75/75 [==============================] - 0s 966us/step - loss: 6.3300e-04 - acc: 0.0133 - val_loss: 5.4682e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2273e-04 - acc: 0.0133 - val_loss: 5.4428e-04 - val_acc: 0.0000e+00\n",
      "Epoch 196/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2003e-04 - acc: 0.0133 - val_loss: 5.4254e-04 - val_acc: 0.0000e+00\n",
      "Epoch 197/382\n",
      "75/75 [==============================] - 0s 973us/step - loss: 6.3147e-04 - acc: 0.0133 - val_loss: 5.3908e-04 - val_acc: 0.0000e+00\n",
      "Epoch 198/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2154e-04 - acc: 0.0133 - val_loss: 5.4753e-04 - val_acc: 0.0000e+00\n",
      "Epoch 199/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2338e-04 - acc: 0.0133 - val_loss: 5.3489e-04 - val_acc: 0.0000e+00\n",
      "Epoch 200/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2007e-04 - acc: 0.0133 - val_loss: 5.3740e-04 - val_acc: 0.0000e+00\n",
      "Epoch 201/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2096e-04 - acc: 0.0133 - val_loss: 5.2637e-04 - val_acc: 0.0000e+00\n",
      "Epoch 202/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1839e-04 - acc: 0.0133 - val_loss: 5.2611e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2307e-04 - acc: 0.0133 - val_loss: 5.2807e-04 - val_acc: 0.0000e+00\n",
      "Epoch 204/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1762e-04 - acc: 0.0133 - val_loss: 5.3165e-04 - val_acc: 0.0000e+00\n",
      "Epoch 205/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2055e-04 - acc: 0.0133 - val_loss: 5.3888e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2075e-04 - acc: 0.0133 - val_loss: 5.3475e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1846e-04 - acc: 0.0133 - val_loss: 5.4136e-04 - val_acc: 0.0000e+00\n",
      "Epoch 208/382\n",
      "75/75 [==============================] - 0s 998us/step - loss: 6.4117e-04 - acc: 0.0133 - val_loss: 5.4735e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/382\n",
      "75/75 [==============================] - 0s 996us/step - loss: 6.2047e-04 - acc: 0.0133 - val_loss: 5.3295e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1541e-04 - acc: 0.0133 - val_loss: 5.3503e-04 - val_acc: 0.0000e+00\n",
      "Epoch 211/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1579e-04 - acc: 0.0133 - val_loss: 5.4538e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2451e-04 - acc: 0.0133 - val_loss: 5.3361e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2960e-04 - acc: 0.0133 - val_loss: 5.2335e-04 - val_acc: 0.0000e+00\n",
      "Epoch 214/382\n",
      "75/75 [==============================] - 0s 963us/step - loss: 6.1706e-04 - acc: 0.0133 - val_loss: 5.4027e-04 - val_acc: 0.0000e+00\n",
      "Epoch 215/382\n",
      "75/75 [==============================] - 0s 995us/step - loss: 6.2402e-04 - acc: 0.0133 - val_loss: 5.5505e-04 - val_acc: 0.0000e+00\n",
      "Epoch 216/382\n",
      "75/75 [==============================] - 0s 982us/step - loss: 6.2891e-04 - acc: 0.0133 - val_loss: 5.3936e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1363e-04 - acc: 0.0133 - val_loss: 5.2154e-04 - val_acc: 0.0000e+00\n",
      "Epoch 218/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0902e-04 - acc: 0.0133 - val_loss: 5.2317e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1527e-04 - acc: 0.0133 - val_loss: 5.4164e-04 - val_acc: 0.0000e+00\n",
      "Epoch 220/382\n",
      "75/75 [==============================] - 0s 970us/step - loss: 6.3006e-04 - acc: 0.0133 - val_loss: 5.1497e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1170e-04 - acc: 0.0133 - val_loss: 5.1692e-04 - val_acc: 0.0000e+00\n",
      "Epoch 222/382\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 6.1166e-04 - acc: 0.0133 - val_loss: 5.1191e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/382\n",
      "75/75 [==============================] - 0s 971us/step - loss: 6.0858e-04 - acc: 0.0133 - val_loss: 5.1967e-04 - val_acc: 0.0000e+00\n",
      "Epoch 224/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1710e-04 - acc: 0.0133 - val_loss: 5.1712e-04 - val_acc: 0.0000e+00\n",
      "Epoch 225/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1815e-04 - acc: 0.0133 - val_loss: 5.2079e-04 - val_acc: 0.0000e+00\n",
      "Epoch 226/382\n",
      "75/75 [==============================] - 0s 975us/step - loss: 6.0696e-04 - acc: 0.0133 - val_loss: 5.2976e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 6.1227e-04 - acc: 0.0133 - val_loss: 5.4435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.2875e-04 - acc: 0.0133 - val_loss: 5.1935e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.1141e-04 - acc: 0.0133 - val_loss: 5.1362e-04 - val_acc: 0.0000e+00\n",
      "Epoch 230/382\n",
      "75/75 [==============================] - 0s 994us/step - loss: 6.0707e-04 - acc: 0.0133 - val_loss: 5.0718e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/382\n",
      "75/75 [==============================] - 0s 984us/step - loss: 6.0488e-04 - acc: 0.0133 - val_loss: 5.1177e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/382\n",
      "75/75 [==============================] - 0s 952us/step - loss: 6.2262e-04 - acc: 0.0133 - val_loss: 5.2096e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 6.0894e-04 - acc: 0.0133 - val_loss: 5.0740e-04 - val_acc: 0.0000e+00\n",
      "Epoch 234/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0752e-04 - acc: 0.0133 - val_loss: 5.1006e-04 - val_acc: 0.0000e+00\n",
      "Epoch 235/382\n",
      "75/75 [==============================] - 0s 992us/step - loss: 6.0060e-04 - acc: 0.0133 - val_loss: 5.0875e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/382\n",
      "75/75 [==============================] - 0s 998us/step - loss: 6.0460e-04 - acc: 0.0133 - val_loss: 5.0912e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0121e-04 - acc: 0.0133 - val_loss: 5.1193e-04 - val_acc: 0.0000e+00\n",
      "Epoch 238/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0022e-04 - acc: 0.0133 - val_loss: 5.0469e-04 - val_acc: 0.0000e+00\n",
      "Epoch 239/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9941e-04 - acc: 0.0133 - val_loss: 5.0362e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0200e-04 - acc: 0.0133 - val_loss: 5.0055e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/382\n",
      "75/75 [==============================] - 0s 977us/step - loss: 6.0279e-04 - acc: 0.0133 - val_loss: 5.0741e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/382\n",
      "75/75 [==============================] - 0s 951us/step - loss: 6.1005e-04 - acc: 0.0133 - val_loss: 5.1029e-04 - val_acc: 0.0000e+00\n",
      "Epoch 243/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0336e-04 - acc: 0.0133 - val_loss: 5.0152e-04 - val_acc: 0.0000e+00\n",
      "Epoch 244/382\n",
      "75/75 [==============================] - 0s 989us/step - loss: 5.9978e-04 - acc: 0.0133 - val_loss: 5.0260e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/382\n",
      "75/75 [==============================] - 0s 971us/step - loss: 5.9787e-04 - acc: 0.0133 - val_loss: 5.0630e-04 - val_acc: 0.0000e+00\n",
      "Epoch 246/382\n",
      "75/75 [==============================] - 0s 948us/step - loss: 6.0075e-04 - acc: 0.0133 - val_loss: 4.9377e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/382\n",
      "75/75 [==============================] - 0s 981us/step - loss: 6.1124e-04 - acc: 0.0133 - val_loss: 5.0316e-04 - val_acc: 0.0000e+00\n",
      "Epoch 248/382\n",
      "75/75 [==============================] - 0s 961us/step - loss: 6.0836e-04 - acc: 0.0133 - val_loss: 5.0682e-04 - val_acc: 0.0000e+00\n",
      "Epoch 249/382\n",
      "75/75 [==============================] - 0s 982us/step - loss: 6.1409e-04 - acc: 0.0133 - val_loss: 5.2493e-04 - val_acc: 0.0000e+00\n",
      "Epoch 250/382\n",
      "75/75 [==============================] - 0s 958us/step - loss: 6.1221e-04 - acc: 0.0133 - val_loss: 5.0080e-04 - val_acc: 0.0000e+00\n",
      "Epoch 251/382\n",
      "75/75 [==============================] - 0s 995us/step - loss: 5.9412e-04 - acc: 0.0133 - val_loss: 4.9822e-04 - val_acc: 0.0000e+00\n",
      "Epoch 252/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0045e-04 - acc: 0.0133 - val_loss: 4.9509e-04 - val_acc: 0.0000e+00\n",
      "Epoch 253/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9316e-04 - acc: 0.0133 - val_loss: 4.9718e-04 - val_acc: 0.0000e+00\n",
      "Epoch 254/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9302e-04 - acc: 0.0133 - val_loss: 4.9729e-04 - val_acc: 0.0000e+00\n",
      "Epoch 255/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9616e-04 - acc: 0.0133 - val_loss: 4.9685e-04 - val_acc: 0.0000e+00\n",
      "Epoch 256/382\n",
      "75/75 [==============================] - 0s 971us/step - loss: 5.9099e-04 - acc: 0.0133 - val_loss: 4.9947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 257/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 5.9559e-04 - acc: 0.0133 - val_loss: 4.9187e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/382\n",
      "75/75 [==============================] - 0s 983us/step - loss: 6.0563e-04 - acc: 0.0133 - val_loss: 5.0441e-04 - val_acc: 0.0000e+00\n",
      "Epoch 259/382\n",
      "75/75 [==============================] - 0s 996us/step - loss: 6.0030e-04 - acc: 0.0133 - val_loss: 4.9435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 260/382\n",
      "75/75 [==============================] - 0s 988us/step - loss: 5.9269e-04 - acc: 0.0133 - val_loss: 4.8869e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 5.9242e-04 - acc: 0.0133 - val_loss: 4.9485e-04 - val_acc: 0.0000e+00\n",
      "Epoch 262/382\n",
      "75/75 [==============================] - 0s 955us/step - loss: 5.9470e-04 - acc: 0.0133 - val_loss: 4.8644e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/382\n",
      "75/75 [==============================] - 0s 987us/step - loss: 5.9625e-04 - acc: 0.0133 - val_loss: 4.8980e-04 - val_acc: 0.0000e+00\n",
      "Epoch 264/382\n",
      "75/75 [==============================] - 0s 977us/step - loss: 5.9783e-04 - acc: 0.0133 - val_loss: 4.8271e-04 - val_acc: 0.0000e+00\n",
      "Epoch 265/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8964e-04 - acc: 0.0133 - val_loss: 4.8511e-04 - val_acc: 0.0000e+00\n",
      "Epoch 266/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8985e-04 - acc: 0.0133 - val_loss: 4.8832e-04 - val_acc: 0.0000e+00\n",
      "Epoch 267/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9131e-04 - acc: 0.0133 - val_loss: 4.9004e-04 - val_acc: 0.0000e+00\n",
      "Epoch 268/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8916e-04 - acc: 0.0133 - val_loss: 4.9751e-04 - val_acc: 0.0000e+00\n",
      "Epoch 269/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9809e-04 - acc: 0.0133 - val_loss: 4.8744e-04 - val_acc: 0.0000e+00\n",
      "Epoch 270/382\n",
      "75/75 [==============================] - 0s 955us/step - loss: 5.8772e-04 - acc: 0.0133 - val_loss: 4.8438e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/382\n",
      "75/75 [==============================] - 0s 953us/step - loss: 5.8984e-04 - acc: 0.0133 - val_loss: 4.8158e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/382\n",
      "75/75 [==============================] - 0s 999us/step - loss: 5.8674e-04 - acc: 0.0133 - val_loss: 4.8192e-04 - val_acc: 0.0000e+00\n",
      "Epoch 273/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8666e-04 - acc: 0.0133 - val_loss: 4.8875e-04 - val_acc: 0.0000e+00\n",
      "Epoch 274/382\n",
      "75/75 [==============================] - 0s 998us/step - loss: 5.9012e-04 - acc: 0.0133 - val_loss: 4.7879e-04 - val_acc: 0.0000e+00\n",
      "Epoch 275/382\n",
      "75/75 [==============================] - 0s 979us/step - loss: 5.9131e-04 - acc: 0.0133 - val_loss: 4.8772e-04 - val_acc: 0.0000e+00\n",
      "Epoch 276/382\n",
      "75/75 [==============================] - 0s 953us/step - loss: 5.9074e-04 - acc: 0.0133 - val_loss: 4.8595e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/382\n",
      "75/75 [==============================] - 0s 954us/step - loss: 5.9367e-04 - acc: 0.0133 - val_loss: 4.9736e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8812e-04 - acc: 0.0133 - val_loss: 4.7613e-04 - val_acc: 0.0000e+00\n",
      "Epoch 279/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8597e-04 - acc: 0.0133 - val_loss: 4.8052e-04 - val_acc: 0.0000e+00\n",
      "Epoch 280/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 5.8911e-04 - acc: 0.0133 - val_loss: 4.8456e-04 - val_acc: 0.0000e+00\n",
      "Epoch 281/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8278e-04 - acc: 0.0133 - val_loss: 4.8009e-04 - val_acc: 0.0000e+00\n",
      "Epoch 282/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8580e-04 - acc: 0.0133 - val_loss: 4.9345e-04 - val_acc: 0.0000e+00\n",
      "Epoch 283/382\n",
      "75/75 [==============================] - 0s 955us/step - loss: 5.8815e-04 - acc: 0.0133 - val_loss: 4.8209e-04 - val_acc: 0.0000e+00\n",
      "Epoch 284/382\n",
      "75/75 [==============================] - 0s 946us/step - loss: 5.8998e-04 - acc: 0.0133 - val_loss: 5.0084e-04 - val_acc: 0.0000e+00\n",
      "Epoch 285/382\n",
      "75/75 [==============================] - 0s 952us/step - loss: 5.8897e-04 - acc: 0.0133 - val_loss: 4.8695e-04 - val_acc: 0.0000e+00\n",
      "Epoch 286/382\n",
      "75/75 [==============================] - 0s 952us/step - loss: 5.8489e-04 - acc: 0.0133 - val_loss: 4.9291e-04 - val_acc: 0.0000e+00\n",
      "Epoch 287/382\n",
      "75/75 [==============================] - 0s 973us/step - loss: 5.8445e-04 - acc: 0.0133 - val_loss: 4.7892e-04 - val_acc: 0.0000e+00\n",
      "Epoch 288/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8223e-04 - acc: 0.0133 - val_loss: 4.9324e-04 - val_acc: 0.0000e+00\n",
      "Epoch 289/382\n",
      "75/75 [==============================] - 0s 969us/step - loss: 5.9778e-04 - acc: 0.0133 - val_loss: 4.8953e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/382\n",
      "75/75 [==============================] - 0s 978us/step - loss: 5.8892e-04 - acc: 0.0133 - val_loss: 4.7625e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/382\n",
      "75/75 [==============================] - 0s 964us/step - loss: 5.7812e-04 - acc: 0.0133 - val_loss: 4.7611e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7785e-04 - acc: 0.0133 - val_loss: 4.7102e-04 - val_acc: 0.0000e+00\n",
      "Epoch 293/382\n",
      "75/75 [==============================] - 0s 987us/step - loss: 5.7808e-04 - acc: 0.0133 - val_loss: 4.7224e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7871e-04 - acc: 0.0133 - val_loss: 4.8368e-04 - val_acc: 0.0000e+00\n",
      "Epoch 295/382\n",
      "75/75 [==============================] - 0s 972us/step - loss: 5.8965e-04 - acc: 0.0133 - val_loss: 4.8844e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/382\n",
      "75/75 [==============================] - 0s 958us/step - loss: 5.9569e-04 - acc: 0.0133 - val_loss: 4.7465e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.0041e-04 - acc: 0.0133 - val_loss: 4.7417e-04 - val_acc: 0.0000e+00\n",
      "Epoch 298/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8172e-04 - acc: 0.0133 - val_loss: 4.6786e-04 - val_acc: 0.0000e+00\n",
      "Epoch 299/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9429e-04 - acc: 0.0133 - val_loss: 4.7383e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/382\n",
      "75/75 [==============================] - 0s 952us/step - loss: 5.8498e-04 - acc: 0.0133 - val_loss: 4.9737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 301/382\n",
      "75/75 [==============================] - 0s 975us/step - loss: 5.9970e-04 - acc: 0.0133 - val_loss: 4.9536e-04 - val_acc: 0.0000e+00\n",
      "Epoch 302/382\n",
      "75/75 [==============================] - 0s 995us/step - loss: 5.8958e-04 - acc: 0.0133 - val_loss: 4.6699e-04 - val_acc: 0.0000e+00\n",
      "Epoch 303/382\n",
      "75/75 [==============================] - 0s 959us/step - loss: 5.7673e-04 - acc: 0.0133 - val_loss: 4.8484e-04 - val_acc: 0.0000e+00\n",
      "Epoch 304/382\n",
      "75/75 [==============================] - 0s 971us/step - loss: 5.8091e-04 - acc: 0.0133 - val_loss: 4.6710e-04 - val_acc: 0.0000e+00\n",
      "Epoch 305/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7264e-04 - acc: 0.0133 - val_loss: 4.6204e-04 - val_acc: 0.0000e+00\n",
      "Epoch 306/382\n",
      "75/75 [==============================] - 0s 994us/step - loss: 5.8184e-04 - acc: 0.0133 - val_loss: 4.6433e-04 - val_acc: 0.0000e+00\n",
      "Epoch 307/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7451e-04 - acc: 0.0133 - val_loss: 4.6308e-04 - val_acc: 0.0000e+00\n",
      "Epoch 308/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 5.7272e-04 - acc: 0.0133 - val_loss: 4.6086e-04 - val_acc: 0.0000e+00\n",
      "Epoch 309/382\n",
      "75/75 [==============================] - 0s 969us/step - loss: 5.7372e-04 - acc: 0.0133 - val_loss: 4.6510e-04 - val_acc: 0.0000e+00\n",
      "Epoch 310/382\n",
      "75/75 [==============================] - 0s 968us/step - loss: 5.7687e-04 - acc: 0.0133 - val_loss: 4.6037e-04 - val_acc: 0.0000e+00\n",
      "Epoch 311/382\n",
      "75/75 [==============================] - 0s 982us/step - loss: 5.7031e-04 - acc: 0.0133 - val_loss: 4.6086e-04 - val_acc: 0.0000e+00\n",
      "Epoch 312/382\n",
      "75/75 [==============================] - 0s 972us/step - loss: 5.7283e-04 - acc: 0.0133 - val_loss: 4.6656e-04 - val_acc: 0.0000e+00\n",
      "Epoch 313/382\n",
      "75/75 [==============================] - 0s 970us/step - loss: 5.7295e-04 - acc: 0.0133 - val_loss: 4.6676e-04 - val_acc: 0.0000e+00\n",
      "Epoch 314/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9374e-04 - acc: 0.0133 - val_loss: 4.7422e-04 - val_acc: 0.0000e+00\n",
      "Epoch 315/382\n",
      "75/75 [==============================] - 0s 981us/step - loss: 5.7490e-04 - acc: 0.0133 - val_loss: 4.7111e-04 - val_acc: 0.0000e+00\n",
      "Epoch 316/382\n",
      "75/75 [==============================] - 0s 982us/step - loss: 5.8095e-04 - acc: 0.0133 - val_loss: 4.6831e-04 - val_acc: 0.0000e+00\n",
      "Epoch 317/382\n",
      "75/75 [==============================] - 0s 963us/step - loss: 5.7605e-04 - acc: 0.0133 - val_loss: 4.6120e-04 - val_acc: 0.0000e+00\n",
      "Epoch 318/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6944e-04 - acc: 0.0133 - val_loss: 4.6480e-04 - val_acc: 0.0000e+00\n",
      "Epoch 319/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7539e-04 - acc: 0.0133 - val_loss: 4.6409e-04 - val_acc: 0.0000e+00\n",
      "Epoch 320/382\n",
      "75/75 [==============================] - 0s 983us/step - loss: 5.7263e-04 - acc: 0.0133 - val_loss: 4.5883e-04 - val_acc: 0.0000e+00\n",
      "Epoch 321/382\n",
      "75/75 [==============================] - 0s 991us/step - loss: 5.6960e-04 - acc: 0.0133 - val_loss: 4.6412e-04 - val_acc: 0.0000e+00\n",
      "Epoch 322/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 5.8265e-04 - acc: 0.0133 - val_loss: 4.5517e-04 - val_acc: 0.0000e+00\n",
      "Epoch 323/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7053e-04 - acc: 0.0133 - val_loss: 4.5847e-04 - val_acc: 0.0000e+00\n",
      "Epoch 324/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7875e-04 - acc: 0.0133 - val_loss: 4.5452e-04 - val_acc: 0.0000e+00\n",
      "Epoch 325/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 5.7054e-04 - acc: 0.0133 - val_loss: 4.7053e-04 - val_acc: 0.0000e+00\n",
      "Epoch 326/382\n",
      "75/75 [==============================] - 0s 988us/step - loss: 5.6851e-04 - acc: 0.0133 - val_loss: 4.6754e-04 - val_acc: 0.0000e+00\n",
      "Epoch 327/382\n",
      "75/75 [==============================] - 0s 989us/step - loss: 6.0519e-04 - acc: 0.0133 - val_loss: 4.8146e-04 - val_acc: 0.0000e+00\n",
      "Epoch 328/382\n",
      "75/75 [==============================] - 0s 986us/step - loss: 5.8574e-04 - acc: 0.0133 - val_loss: 4.5658e-04 - val_acc: 0.0000e+00\n",
      "Epoch 329/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8414e-04 - acc: 0.0133 - val_loss: 4.5328e-04 - val_acc: 0.0000e+00\n",
      "Epoch 330/382\n",
      "75/75 [==============================] - 0s 976us/step - loss: 5.6728e-04 - acc: 0.0133 - val_loss: 4.5086e-04 - val_acc: 0.0000e+00\n",
      "Epoch 331/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 5.6529e-04 - acc: 0.0133 - val_loss: 4.5894e-04 - val_acc: 0.0000e+00\n",
      "Epoch 332/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7877e-04 - acc: 0.0133 - val_loss: 4.6794e-04 - val_acc: 0.0000e+00\n",
      "Epoch 333/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7590e-04 - acc: 0.0133 - val_loss: 4.5214e-04 - val_acc: 0.0000e+00\n",
      "Epoch 334/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6836e-04 - acc: 0.0133 - val_loss: 4.6580e-04 - val_acc: 0.0000e+00\n",
      "Epoch 335/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7360e-04 - acc: 0.0133 - val_loss: 4.4680e-04 - val_acc: 0.0000e+00\n",
      "Epoch 336/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6870e-04 - acc: 0.0133 - val_loss: 4.4721e-04 - val_acc: 0.0000e+00\n",
      "Epoch 337/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7835e-04 - acc: 0.0133 - val_loss: 4.5928e-04 - val_acc: 0.0000e+00\n",
      "Epoch 338/382\n",
      "75/75 [==============================] - 0s 985us/step - loss: 5.7939e-04 - acc: 0.0133 - val_loss: 4.4424e-04 - val_acc: 0.0000e+00\n",
      "Epoch 339/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6233e-04 - acc: 0.0133 - val_loss: 4.4017e-04 - val_acc: 0.0000e+00\n",
      "Epoch 340/382\n",
      "75/75 [==============================] - 0s 969us/step - loss: 5.6450e-04 - acc: 0.0133 - val_loss: 4.3729e-04 - val_acc: 0.0000e+00\n",
      "Epoch 341/382\n",
      "75/75 [==============================] - 0s 974us/step - loss: 5.6215e-04 - acc: 0.0133 - val_loss: 4.3872e-04 - val_acc: 0.0000e+00\n",
      "Epoch 342/382\n",
      "75/75 [==============================] - 0s 987us/step - loss: 5.6326e-04 - acc: 0.0133 - val_loss: 4.4293e-04 - val_acc: 0.0000e+00\n",
      "Epoch 343/382\n",
      "75/75 [==============================] - 0s 977us/step - loss: 5.6655e-04 - acc: 0.0133 - val_loss: 4.4096e-04 - val_acc: 0.0000e+00\n",
      "Epoch 344/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6591e-04 - acc: 0.0133 - val_loss: 4.4490e-04 - val_acc: 0.0000e+00\n",
      "Epoch 345/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6301e-04 - acc: 0.0133 - val_loss: 4.4348e-04 - val_acc: 0.0000e+00\n",
      "Epoch 346/382\n",
      "75/75 [==============================] - 0s 997us/step - loss: 5.6482e-04 - acc: 0.0133 - val_loss: 4.6072e-04 - val_acc: 0.0000e+00\n",
      "Epoch 347/382\n",
      "75/75 [==============================] - 0s 970us/step - loss: 5.6635e-04 - acc: 0.0133 - val_loss: 4.4127e-04 - val_acc: 0.0000e+00\n",
      "Epoch 348/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.9715e-04 - acc: 0.0133 - val_loss: 4.4079e-04 - val_acc: 0.0000e+00\n",
      "Epoch 349/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6332e-04 - acc: 0.0133 - val_loss: 4.6122e-04 - val_acc: 0.0000e+00\n",
      "Epoch 350/382\n",
      "75/75 [==============================] - 0s 977us/step - loss: 5.8723e-04 - acc: 0.0133 - val_loss: 4.7264e-04 - val_acc: 0.0000e+00\n",
      "Epoch 351/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7163e-04 - acc: 0.0133 - val_loss: 4.5323e-04 - val_acc: 0.0000e+00\n",
      "Epoch 352/382\n",
      "75/75 [==============================] - 0s 963us/step - loss: 5.6071e-04 - acc: 0.0133 - val_loss: 4.4481e-04 - val_acc: 0.0000e+00\n",
      "Epoch 353/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6108e-04 - acc: 0.0133 - val_loss: 4.3839e-04 - val_acc: 0.0000e+00\n",
      "Epoch 354/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6000e-04 - acc: 0.0133 - val_loss: 4.5569e-04 - val_acc: 0.0000e+00\n",
      "Epoch 355/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6907e-04 - acc: 0.0133 - val_loss: 4.7525e-04 - val_acc: 0.0000e+00\n",
      "Epoch 356/382\n",
      "75/75 [==============================] - 0s 976us/step - loss: 5.8032e-04 - acc: 0.0133 - val_loss: 4.3607e-04 - val_acc: 0.0000e+00\n",
      "Epoch 357/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6191e-04 - acc: 0.0133 - val_loss: 4.4228e-04 - val_acc: 0.0000e+00\n",
      "Epoch 358/382\n",
      "75/75 [==============================] - 0s 981us/step - loss: 5.6824e-04 - acc: 0.0133 - val_loss: 4.3811e-04 - val_acc: 0.0000e+00\n",
      "Epoch 359/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7351e-04 - acc: 0.0133 - val_loss: 4.3774e-04 - val_acc: 0.0000e+00\n",
      "Epoch 360/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6646e-04 - acc: 0.0133 - val_loss: 4.4496e-04 - val_acc: 0.0000e+00\n",
      "Epoch 361/382\n",
      "75/75 [==============================] - 0s 984us/step - loss: 5.6602e-04 - acc: 0.0133 - val_loss: 4.3157e-04 - val_acc: 0.0000e+00\n",
      "Epoch 362/382\n",
      "75/75 [==============================] - 0s 990us/step - loss: 5.6416e-04 - acc: 0.0133 - val_loss: 4.3626e-04 - val_acc: 0.0000e+00\n",
      "Epoch 363/382\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 5.6204e-04 - acc: 0.0133 - val_loss: 4.3211e-04 - val_acc: 0.0000e+00\n",
      "Epoch 364/382\n",
      "75/75 [==============================] - 0s 994us/step - loss: 5.6068e-04 - acc: 0.0133 - val_loss: 4.3217e-04 - val_acc: 0.0000e+00\n",
      "Epoch 365/382\n",
      "75/75 [==============================] - 0s 972us/step - loss: 5.5567e-04 - acc: 0.0133 - val_loss: 4.3852e-04 - val_acc: 0.0000e+00\n",
      "Epoch 366/382\n",
      "75/75 [==============================] - 0s 987us/step - loss: 5.5712e-04 - acc: 0.0133 - val_loss: 4.7353e-04 - val_acc: 0.0000e+00\n",
      "Epoch 367/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6883e-04 - acc: 0.0133 - val_loss: 4.4210e-04 - val_acc: 0.0000e+00\n",
      "Epoch 368/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5752e-04 - acc: 0.0133 - val_loss: 4.6271e-04 - val_acc: 0.0000e+00\n",
      "Epoch 369/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7251e-04 - acc: 0.0133 - val_loss: 4.4537e-04 - val_acc: 0.0000e+00\n",
      "Epoch 370/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.6083e-04 - acc: 0.0133 - val_loss: 4.4452e-04 - val_acc: 0.0000e+00\n",
      "Epoch 371/382\n",
      "75/75 [==============================] - 0s 965us/step - loss: 5.5731e-04 - acc: 0.0133 - val_loss: 4.3434e-04 - val_acc: 0.0000e+00\n",
      "Epoch 372/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.7310e-04 - acc: 0.0133 - val_loss: 4.3236e-04 - val_acc: 0.0000e+00\n",
      "Epoch 373/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5330e-04 - acc: 0.0133 - val_loss: 4.3382e-04 - val_acc: 0.0000e+00\n",
      "Epoch 374/382\n",
      "75/75 [==============================] - 0s 993us/step - loss: 5.5241e-04 - acc: 0.0133 - val_loss: 4.3283e-04 - val_acc: 0.0000e+00\n",
      "Epoch 375/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5742e-04 - acc: 0.0133 - val_loss: 4.3046e-04 - val_acc: 0.0000e+00\n",
      "Epoch 376/382\n",
      "75/75 [==============================] - 0s 980us/step - loss: 5.6292e-04 - acc: 0.0133 - val_loss: 4.5100e-04 - val_acc: 0.0000e+00\n",
      "Epoch 377/382\n",
      "75/75 [==============================] - 0s 959us/step - loss: 5.6631e-04 - acc: 0.0133 - val_loss: 4.2679e-04 - val_acc: 0.0000e+00\n",
      "Epoch 378/382\n",
      "75/75 [==============================] - 0s 958us/step - loss: 5.5218e-04 - acc: 0.0133 - val_loss: 4.2554e-04 - val_acc: 0.0000e+00\n",
      "Epoch 379/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5431e-04 - acc: 0.0133 - val_loss: 4.2580e-04 - val_acc: 0.0000e+00\n",
      "Epoch 380/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5815e-04 - acc: 0.0133 - val_loss: 4.2630e-04 - val_acc: 0.0000e+00\n",
      "Epoch 381/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5234e-04 - acc: 0.0133 - val_loss: 4.2711e-04 - val_acc: 0.0000e+00\n",
      "Epoch 382/382\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.5728e-04 - acc: 0.0133 - val_loss: 4.2672e-04 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=382, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wd1NwFapk0Ro"
   },
   "outputs": [],
   "source": [
    "results = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1562898531748,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "flaF9gxnvBVq",
    "outputId": "0bdcb484-da99-47b6-ca6c-8d12c8b665ce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noP8HnxvbAF4+0ucAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(25), results, c='red')\n",
    "plt.scatter(range(25), y_test, c='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Sx8LumkL_vh"
   },
   "source": [
    "Revisamos una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1562898534237,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "kknzlM4J1bSE",
    "outputId": "92684f05-4de6-46c1-a401-f72a0b4eb244"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.83],\n",
       "        [0.84],\n",
       "        [0.85],\n",
       "        [0.86],\n",
       "        [0.87]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1562898537368,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "olrS5GFg2xEL",
    "outputId": "9a2c7d0e-3822-452b-c113-31a56c637092"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1562898539175,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "kfVN05ny213T",
    "outputId": "9e571d78-1bc7-44d3-b304-b7e9a7fc34e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88517964]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eggd7pVMMC38"
   },
   "source": [
    "Comprobamos con una nueva entrada completamente desconocida para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1562898544209,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "dbMW2741Koft",
    "outputId": "83877310-35a4-4af5-f55f-09357333b8e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.02],\n",
       "        [0.04],\n",
       "        [0.06],\n",
       "        [0.08],\n",
       "        [0.1 ]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = np.array([[[2],\n",
    "                    [4],\n",
    "                    [6],\n",
    "                    [8],\n",
    "                    [10]]])/100 # Dividir en 100 para normalizar\n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1562898547743,
     "user": {
      "displayName": "Anni Alejandra Piragauta Urrea",
      "photoUrl": "https://lh5.googleusercontent.com/-iEUBFNaFH-0/AAAAAAAAAAI/AAAAAAAAAAo/JGyG98Ab3Hs/s64/photo.jpg",
      "userId": "07208906637070303459"
     },
     "user_tz": 300
    },
    "id": "QBj9CY9sLGH-",
    "outputId": "62db4feb-6e75-4b37-ef48-3e268758de48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.7435665]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_predict = model.predict(n_test)\n",
    "n_predict*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u442u05IA2HK"
   },
   "source": [
    "**Comentarios:** Durante este taller se utiliza una Red Neuronal Recurrente para predecir el n√∫mero que sigue dado un conjunto de n√∫meros, dada la entrada [2,4,6,8] se obtuvo 12."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "06_RNN",
   "provenance": [
    {
     "file_id": "1Io4_LEy88NCZ77fLOriA6HbHeD_4nhGJ",
     "timestamp": 1558979395582
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
